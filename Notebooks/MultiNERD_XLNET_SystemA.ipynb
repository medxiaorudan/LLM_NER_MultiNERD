{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition Task SystemA (MultiNERD Dataset)\n",
    "\n",
    "### XLNET Base Cased\n",
    "\n",
    "[MultiNERD Dataset] ðŸ¤—: https://huggingface.co/datasets/Babelscape/multinerd \n",
    "\n",
    "[xlnet Base Cased Model] ðŸ¤—: https://huggingface.co/xlnet-base-cased"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries\n",
    "\n",
    "* [AutoTokenizer](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer): A tokenizer class designed to accommodate the tokenization conventions of various pre-trained models.\n",
    "\n",
    "* [AutoModelForTokenClassification](https://huggingface.co/transformers/model_doc/auto.html#automodelfortokenclassification): An extension of the [AutoModel](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel) class, capable of loading diverse pre-trained models. It supports fine-tuning for the classification of each token within a sequence.\n",
    "\n",
    "* [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments): A straightforward class tailored for storing hyperparameters and other settings essential for model training.\n",
    "\n",
    "* [Trainer](https://huggingface.co/transformers/main_classes/trainer.html): A versatile class that facilitates various forms of training for transformer models.\n",
    "\n",
    "* [DataCollatorForTokenClassification](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py): A class designed for padding token classification examples to the same length during training.\n",
    "\n",
    "* [load_dataset](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset): A function crafted for effortlessly loading datasets, including those from the [datasets](https://huggingface.co/datasets) collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting English Subset of Dataset as input of SystemA\n",
    "\n",
    "* First load the dataset with [`load_dataset`](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset), Then Filter out the non-English examples of the dataset as the training dataset of the SystemA.\n",
    "\n",
    "* System A supposed to be a fine-tuned XLNET model on the English subset of the training set.\n",
    "\n",
    "* The loaded dataset is a dictionary-like container for [`Dataset`](https://huggingface.co/docs/datasets/exploring.html) objects for training, development (validation), and test data. We'll here be using the `tokens` and `ner_tags` fields and ignoring `lang` (language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:04<00:00,  4.68it/s]\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 144382.24it/s]\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 182361.04it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_dataset = load_dataset('Babelscape/multinerd')\n",
    "data = original_dataset.filter(lambda example: example['lang'] == 'en')\n",
    "data = data.remove_columns([\"lang\"])\n",
    "label_list = ['O',\n",
    " 'B-EVE',\n",
    " 'I-EVE',\n",
    " 'B-LOC',\n",
    " 'B-MEDIA',\n",
    " 'I-MEDIA',\n",
    " 'I-PER',\n",
    " 'B-PER',\n",
    " 'B-DIS',\n",
    " 'I-LOC',\n",
    " 'B-PLANT',\n",
    " 'B-FOOD',\n",
    " 'B-VEHI',\n",
    " 'I-VEHI',\n",
    " 'I-ORG',\n",
    " 'I-FOOD',\n",
    " 'B-ORG',\n",
    " 'B-TIME',\n",
    " 'B-ANIM',\n",
    " 'B-CEL',\n",
    " 'I-TIME',\n",
    " 'B-MYTH',\n",
    " 'I-MYTH',\n",
    " 'I-DIS',\n",
    " 'I-ANIM',\n",
    " 'B-INST',\n",
    " 'I-PLANT',\n",
    " 'B-BIO',\n",
    " 'I-INST',\n",
    " 'I-CEL',\n",
    " 'I-BIO']\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (262560, 2)\n",
      "Validation data shape: (32908, 2)\n",
      "Testing data shape: (32820, 2)\n"
     ]
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    'train': data['train'], \n",
    "    'eval': data['validation'], \n",
    "    'test': data['test']})\n",
    "\n",
    "print('Training data shape:', ds['train'].shape)\n",
    "print('Validation data shape:', ds['eval'].shape)\n",
    "print('Testing data shape:', ds['test'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['The',\n",
       "  'wild',\n",
       "  'bulb',\n",
       "  'vernal',\n",
       "  'squill',\n",
       "  'is',\n",
       "  'known',\n",
       "  'locally',\n",
       "  'as',\n",
       "  '\"',\n",
       "  'grice',\n",
       "  \"'s\",\n",
       "  'onions',\n",
       "  '\"',\n",
       "  'because',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'favourite',\n",
       "  'food',\n",
       "  'of',\n",
       "  'the',\n",
       "  'swine',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  25,\n",
       "  26,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds['train'][12]\n",
    "example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Feature Information About Each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: \n",
      "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "\n",
      "ner_tags: \n",
      "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in ds[\"train\"].features.items():\n",
    "    print(f\"{k}: \\n{v}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Tag Values & Conversions Between String & Integer Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tag values: \n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-ANIM', 'I-ANIM', 'B-BIO', 'I-BIO', 'B-CEL', 'I-CEL', 'B-DIS', 'I-DIS', 'B-EVE', 'I-EVE', 'B-FOOD', 'I-FOOD', 'B-INST', 'I-INST', 'B-MEDIA', 'I-MEDIA', 'B-MYTH', 'I-MYTH', 'B-PLANT', 'I-PLANT', 'B-TIME', 'I-TIME', 'B-VEHI', 'I-VEHI']\n",
      "Number of NER Tags: \n",
      "31\n",
      "id2label: \n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-ANIM', 8: 'I-ANIM', 9: 'B-BIO', 10: 'I-BIO', 11: 'B-CEL', 12: 'I-CEL', 13: 'B-DIS', 14: 'I-DIS', 15: 'B-EVE', 16: 'I-EVE', 17: 'B-FOOD', 18: 'I-FOOD', 19: 'B-INST', 20: 'I-INST', 21: 'B-MEDIA', 22: 'I-MEDIA', 23: 'B-MYTH', 24: 'I-MYTH', 25: 'B-PLANT', 26: 'I-PLANT', 27: 'B-TIME', 28: 'I-TIME', 29: 'B-VEHI', 30: 'I-VEHI'}\n",
      "label2id: \n",
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-ANIM': 7, 'I-ANIM': 8, 'B-BIO': 9, 'I-BIO': 10, 'B-CEL': 11, 'I-CEL': 12, 'B-DIS': 13, 'I-DIS': 14, 'B-EVE': 15, 'I-EVE': 16, 'B-FOOD': 17, 'I-FOOD': 18, 'B-INST': 19, 'I-INST': 20, 'B-MEDIA': 21, 'I-MEDIA': 22, 'B-MYTH': 23, 'I-MYTH': 24, 'B-PLANT': 25, 'I-PLANT': 26, 'B-TIME': 27, 'I-TIME': 28, 'B-VEHI': 29, 'I-VEHI': 30}\n"
     ]
    }
   ],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-ANIM\": 7,\n",
    "    \"I-ANIM\": 8,\n",
    "    \"B-BIO\": 9,\n",
    "    \"I-BIO\": 10,\n",
    "    \"B-CEL\": 11,\n",
    "    \"I-CEL\": 12,\n",
    "    \"B-DIS\": 13,\n",
    "    \"I-DIS\": 14,\n",
    "    \"B-EVE\": 15,\n",
    "    \"I-EVE\": 16,\n",
    "    \"B-FOOD\": 17,\n",
    "    \"I-FOOD\": 18,\n",
    "    \"B-INST\": 19,\n",
    "    \"I-INST\": 20,\n",
    "    \"B-MEDIA\": 21,\n",
    "    \"I-MEDIA\": 22,\n",
    "    \"B-MYTH\": 23,\n",
    "    \"I-MYTH\": 24,\n",
    "    \"B-PLANT\": 25,\n",
    "    \"I-PLANT\": 26,\n",
    "    \"B-TIME\": 27,\n",
    "    \"I-TIME\": 28,\n",
    "    \"B-VEHI\": 29,\n",
    "    \"I-VEHI\": 30,\n",
    "  }\n",
    "\n",
    "id2label = {tag: idx for idx, tag in label2id.items()}\n",
    "\n",
    "pos_tag_values = list(label2id.keys())\n",
    "NUM_OF_LABELS = len(pos_tag_values)\n",
    "\n",
    "print(f\"List of tag values: \\n{pos_tag_values}\")\n",
    "print(f\"Number of NER Tags: \\n{NUM_OF_LABELS}\")\n",
    "print(f\"id2label: \\n{id2label}\")\n",
    "print(f\"label2id: \\n{label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text classification tasks discussed earlier, each data example included a string representing an entire sentence. In contrast, the current data is already segmented into words, denoted by the key `tokens` in the `Dataset` object. Interestingly, the tags in the data correspond to these individual words. Let's examine a sample sentence for better clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The      âž” O\n",
      "type     âž” O\n",
      "locality âž” O\n",
      "is       âž” O\n",
      "KÄ«lauea  âž” I-MEDIA\n",
      ".        âž” O\n"
     ]
    }
   ],
   "source": [
    "train_words = ds['train']['tokens']\n",
    "train_tags = ds['train']['ner_tags']\n",
    "for word, tag in zip(train_words[0], train_tags[0]):\n",
    "    print(f'{word:8s} âž” {label_list[tag]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aligning labels (tags) with words, especially when they don't align perfectly with the tokenization of the model, introduces significant complexity. This becomes more apparent once we've loaded a tokenizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental Constants and Parameters Configuration\n",
    "\n",
    "Let's establish critical global variables encompassing the designation of the pre-trained model and the essential hyperparameters instrumental for its meticulous fine-tuning:\n",
    "\n",
    "* `MODEL_CKPT`: The nomenclature of a pre-trained model accessible within the [model repository](https://huggingface.co/models).\n",
    "* `REPORTS_TO`: Incorporating visualization tools vital for comprehensive machine learning experimentation, opt for [TensorBoard](https://www.tensorflow.org/tensorboard) in this context.\n",
    "* `LR`, `WEIGHT_DECAY`, `BATCH_SIZE`, `STEPS`, and `NUM_OF_EPOCHS`: Configurable hyperparameters governing the nuanced fine-tuning process of the model. (Experimenting with diverse values is encouraged!)\n",
    "* `OUTPUT_DIR` and `LOG_DIR`: The designated directories for storing the model and its associated parameters.\n",
    "* `DEVICE` : The computational device on which the model training is executed. In this instance, utilize GPU for optimal training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "MODEL_CKPT = \"xlnet-base-cased\"\n",
    "MODEL_NAME = f\"{MODEL_CKPT}-finetuned-MultiNERD-SystemA\"\n",
    "\n",
    "NUM_OF_EPOCHS = 2\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "STRATEGY = \"epoch\"\n",
    "REPORTS_TO = \"tensorboard\"\n",
    "\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR = 2e-5\n",
    "\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "STEPS = 1250\n",
    "\n",
    "OUTPUT_DIR = f'/srv/users/rudxia/Developer_NLP/notebooks/results/Outdir/{MODEL_CKPT}-finetuned-MultiNERD-SystemA'\n",
    "LOG_DIR= f'/srv/users/rudxia/Developer_NLP/notebooks/results/Log/{MODEL_CKPT}-finetuned-MultiNERD-SystemA'\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Tokenize & Align Inputs\n",
    "\n",
    "Ensuring the alignment of labels (tags) with words that might not align perfectly with the model involves loading a suitable tokenizer using [`AutoTokenizer.from_pretrained`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained). Specifically, we opt for a [\"fast\" tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html) since it offers a mapping from tokens to input words, a crucial requirement for Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "def tokenize_and_align_labels(samples):\n",
    "    tokenized_inputs = tokenizer(samples[\"tokens\"], \n",
    "                                      truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for idx, label in enumerate(samples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        prev_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids: # set special tokens to -100\n",
    "            if word_idx is None or word_idx == prev_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            prev_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Above Function to Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/32908 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32908/32908 [00:05<00:00, 6129.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_ds = ds.map(tokenize_and_align_labels, \n",
    "                    batched=True, \n",
    "                    remove_columns=\n",
    "                        [\n",
    "                            'ner_tags', \n",
    "                            'tokens'\n",
    "                        ]\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = (AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    num_labels=NUM_OF_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    ).to(DEVICE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = pos_tag_values\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "labels = [label_list[i] for i in example[f'ner_tags']]\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, \n",
    "                            axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, \n",
    "                              references=true_labels)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    OUTPUT_DIR,    # output directory for checkpoints and predictions\n",
    "    MODEL_NAME,\n",
    "    log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    "    logging_dir = LOG_DIR,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=NUM_OF_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    report_to=REPORTS_TO,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    save_strategy=STRATEGY,\n",
    "    hub_private_repo=False,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Trainer and Subclass Trainer to Handle Class Imbalance\n",
    "\n",
    "To fine-tune the pre-trained model, we'll instantiate a [`Trainer`](https://huggingface.co/transformers/main_classes/trainer.html) object. This involves providing the pre-trained model, configuration settings, training and validation datasets, and the previously defined evaluation metric.\n",
    "\n",
    "Most of this should be familiar from the sentence classification notebook, with one notable exception: we include a [`DataCollatorForTokenClassification`](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py) with the trainer. This class efficiently handles the padding of token classification examples in batches, ensuring they are of the same length, as required by PyTorch. The tokenizer's `[PAD]` symbol is employed for padding the output, along with the specified `label_pad_token_id` for padding the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, \n",
    "                     model, \n",
    "                     inputs, \n",
    "                     return_outputs=False):\n",
    "        \n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "            [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \n",
    "             9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0,\n",
    "             16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0,\n",
    "             23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0], \n",
    "            device=DEVICE)\n",
    "        )\n",
    "        loss = loss_fct(logits.view(-1, \n",
    "                                    self.model.config.num_labels), \n",
    "                        labels.view(-1)\n",
    "                        )\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "trainer = CustomTrainer(model, \n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer,\n",
    "                  train_dataset=encoded_ds[\"train\"],\n",
    "                  eval_dataset=encoded_ds[\"eval\"],\n",
    "                  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10940' max='10940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10940/10940 1:06:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Anim</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Cel</th>\n",
       "      <th>Dis</th>\n",
       "      <th>Eve</th>\n",
       "      <th>Food</th>\n",
       "      <th>Inst</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Media</th>\n",
       "      <th>Myth</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Plant</th>\n",
       "      <th>Time</th>\n",
       "      <th>Vehi</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.104612</td>\n",
       "      <td>{'precision': 0.5805658056580566, 'recall': 0.8827930174563591, 'f1': 0.7004699480583725, 'number': 3208}</td>\n",
       "      <td>{'precision': 0.5, 'recall': 0.375, 'f1': 0.42857142857142855, 'number': 16}</td>\n",
       "      <td>{'precision': 0.5535714285714286, 'recall': 0.7560975609756098, 'f1': 0.6391752577319588, 'number': 82}</td>\n",
       "      <td>{'precision': 0.5325301204819277, 'recall': 0.8735177865612648, 'f1': 0.6616766467065868, 'number': 1518}</td>\n",
       "      <td>{'precision': 0.9344262295081968, 'recall': 0.9715909090909091, 'f1': 0.9526462395543176, 'number': 704}</td>\n",
       "      <td>{'precision': 0.384297520661157, 'recall': 0.8215547703180212, 'f1': 0.5236486486486486, 'number': 1132}</td>\n",
       "      <td>{'precision': 0.4444444444444444, 'recall': 0.6666666666666666, 'f1': 0.5333333333333333, 'number': 24}</td>\n",
       "      <td>{'precision': 0.9922686840136338, 'recall': 0.9926813040585496, 'f1': 0.9924749511495448, 'number': 24048}</td>\n",
       "      <td>{'precision': 0.9200819672131147, 'recall': 0.980349344978166, 'f1': 0.9492600422832981, 'number': 916}</td>\n",
       "      <td>{'precision': 0.3835616438356164, 'recall': 0.875, 'f1': 0.5333333333333333, 'number': 64}</td>\n",
       "      <td>{'precision': 0.9717548076923077, 'recall': 0.9773345421577516, 'f1': 0.9745366882627693, 'number': 6618}</td>\n",
       "      <td>{'precision': 0.9925558312655087, 'recall': 0.9876543209876543, 'f1': 0.9900990099009901, 'number': 10530}</td>\n",
       "      <td>{'precision': 0.41868317388857623, 'recall': 0.8322147651006712, 'f1': 0.5570947210782479, 'number': 1788}</td>\n",
       "      <td>{'precision': 0.6090425531914894, 'recall': 0.7923875432525952, 'f1': 0.6887218045112782, 'number': 578}</td>\n",
       "      <td>{'precision': 0.6511627906976745, 'recall': 0.875, 'f1': 0.7466666666666667, 'number': 64}</td>\n",
       "      <td>0.863486</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.911922</td>\n",
       "      <td>0.982975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.115915</td>\n",
       "      <td>{'precision': 0.6152164407520769, 'recall': 0.8771820448877805, 'f1': 0.7232074016962221, 'number': 3208}</td>\n",
       "      <td>{'precision': 0.47058823529411764, 'recall': 1.0, 'f1': 0.6399999999999999, 'number': 16}</td>\n",
       "      <td>{'precision': 0.603448275862069, 'recall': 0.8536585365853658, 'f1': 0.7070707070707071, 'number': 82}</td>\n",
       "      <td>{'precision': 0.5658783783783784, 'recall': 0.8827404479578392, 'f1': 0.6896551724137931, 'number': 1518}</td>\n",
       "      <td>{'precision': 0.9301075268817204, 'recall': 0.9829545454545454, 'f1': 0.9558011049723756, 'number': 704}</td>\n",
       "      <td>{'precision': 0.4458413926499033, 'recall': 0.8144876325088339, 'f1': 0.5762499999999999, 'number': 1132}</td>\n",
       "      <td>{'precision': 0.4117647058823529, 'recall': 0.5833333333333334, 'f1': 0.4827586206896552, 'number': 24}</td>\n",
       "      <td>{'precision': 0.9946795244825006, 'recall': 0.9950931470392548, 'f1': 0.9948862927701326, 'number': 24048}</td>\n",
       "      <td>{'precision': 0.9430379746835443, 'recall': 0.9759825327510917, 'f1': 0.9592274678111588, 'number': 916}</td>\n",
       "      <td>{'precision': 0.6666666666666666, 'recall': 0.875, 'f1': 0.7567567567567567, 'number': 64}</td>\n",
       "      <td>{'precision': 0.9779722389861195, 'recall': 0.9794499848896948, 'f1': 0.9787105541295485, 'number': 6618}</td>\n",
       "      <td>{'precision': 0.9929857819905213, 'recall': 0.9948717948717949, 'f1': 0.9939278937381405, 'number': 10530}</td>\n",
       "      <td>{'precision': 0.46262507357268984, 'recall': 0.8791946308724832, 'f1': 0.6062475896644813, 'number': 1788}</td>\n",
       "      <td>{'precision': 0.6657534246575343, 'recall': 0.8408304498269896, 'f1': 0.7431192660550459, 'number': 578}</td>\n",
       "      <td>{'precision': 0.7631578947368421, 'recall': 0.90625, 'f1': 0.8285714285714286, 'number': 64}</td>\n",
       "      <td>0.883212</td>\n",
       "      <td>0.971378</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.985433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = \"/srv/users/rudxia/Developer_NLP/notebooks/results/saved_model/XLNET_MultiNERD_SystemA\"\n",
    "trainer.save_model(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               = 22355475GF\n",
      "  train_loss               =     0.1292\n",
      "  train_runtime            = 1:06:53.31\n",
      "  train_samples_per_second =    130.845\n",
      "  train_steps_per_second   =      2.726\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model is now available as `trainer.model`. Here we define functions to predict tags for a user-defined string.\n",
    "\n",
    "The main complexity here arises from the need to map back from token labels to word labels, inverting the mapping performed in `encode_dataset`. The process is basically the same as in `compute_metrics` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "model.eval()    # switch to evaluation mode\n",
    "model.to('cpu')    # switch to CPU\n",
    "\n",
    "\n",
    "def word_start_tokens(tokenized):\n",
    "    \"\"\"Return list of bool identifying which tokens start words.\"\"\"\n",
    "    prev_word_idx = None\n",
    "    is_word_start = []\n",
    "    for word_idx in tokenized.word_ids():\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            is_word_start.append(False)\n",
    "        else:\n",
    "            is_word_start.append(True)\n",
    "        prev_word_idx = word_idx\n",
    "    return is_word_start\n",
    "\n",
    "\n",
    "def predict_ner(words):\n",
    "    tokenized = tokenizer(words, is_split_into_words=True, return_tensors='pt')\n",
    "    pred = model(**tokenized)\n",
    "    pred_idx = pred.logits.detach().numpy().argmax(axis=2)\n",
    "    token_labels = [label_list[i] for s in pred_idx for i in s]\n",
    "    word_labels = []\n",
    "    for label, is_word_start in zip(token_labels, word_start_tokens(tokenized)):\n",
    "        if is_word_start:\n",
    "            word_labels.append(label)\n",
    "    return word_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that out on a couple of example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris,     âž” B-LOC\n",
      "the        âž” O\n",
      "capital    âž” O\n",
      "of         âž” O\n",
      "France,    âž” B-LOC\n",
      "is         âž” O\n",
      "known      âž” O\n",
      "for        âž” O\n",
      "its        âž” O\n",
      "iconic     âž” O\n",
      "Eiffel     âž” B-LOC\n",
      "Tower.     âž” I-LOC\n",
      "\n",
      "Elon       âž” B-PER\n",
      "Musk       âž” I-PER\n",
      "is         âž” O\n",
      "the        âž” O\n",
      "CEO        âž” O\n",
      "of         âž” O\n",
      "SpaceX     âž” B-ORG\n",
      "and        âž” O\n",
      "Tesla.     âž” B-ORG\n",
      "\n",
      "The        âž” O\n",
      "Great      âž” B-LOC\n",
      "Barrier    âž” I-LOC\n",
      "Reef       âž” I-LOC\n",
      "is         âž” O\n",
      "the        âž” O\n",
      "world's    âž” O\n",
      "largest    âž” O\n",
      "coral      âž” O\n",
      "reef       âž” O\n",
      "system,    âž” O\n",
      "located    âž” O\n",
      "in         âž” O\n",
      "Australia. âž” B-LOC\n",
      "\n",
      "Mount      âž” B-LOC\n",
      "Everest,   âž” I-LOC\n",
      "the        âž” O\n",
      "highest    âž” O\n",
      "peak       âž” O\n",
      "in         âž” O\n",
      "the        âž” O\n",
      "world,     âž” O\n",
      "is         âž” O\n",
      "part       âž” O\n",
      "of         âž” O\n",
      "the        âž” O\n",
      "Himalayan  âž” B-LOC\n",
      "mountain   âž” O\n",
      "range.     âž” O\n",
      "\n",
      "The        âž” O\n",
      "Mona       âž” B-LOC\n",
      "Lisa,      âž” I-LOC\n",
      "painted    âž” O\n",
      "by         âž” O\n",
      "Leonardo   âž” B-PER\n",
      "da         âž” I-PER\n",
      "Vinci,     âž” I-PER\n",
      "is         âž” O\n",
      "displayed  âž” O\n",
      "in         âž” O\n",
      "the        âž” O\n",
      "Louvre     âž” B-LOC\n",
      "Museum     âž” I-LOC\n",
      "in         âž” O\n",
      "Paris.     âž” B-LOC\n",
      "\n",
      "Barack     âž” B-PER\n",
      "Obama      âž” I-PER\n",
      "served     âž” O\n",
      "as         âž” O\n",
      "the        âž” O\n",
      "44th       âž” O\n",
      "President  âž” O\n",
      "of         âž” O\n",
      "the        âž” O\n",
      "United     âž” B-LOC\n",
      "States     âž” I-LOC\n",
      "from       âž” O\n",
      "2009       âž” O\n",
      "to         âž” O\n",
      "2017.      âž” O\n",
      "\n",
      "The        âž” O\n",
      "Amazon     âž” B-LOC\n",
      "rainforest, âž” I-LOC\n",
      "often      âž” O\n",
      "called     âž” O\n",
      "the        âž” O\n",
      "\"lungs     âž” O\n",
      "of         âž” O\n",
      "the        âž” O\n",
      "Earth,\"    âž” O\n",
      "is         âž” O\n",
      "vital      âž” O\n",
      "for        âž” O\n",
      "global     âž” O\n",
      "oxygen     âž” O\n",
      "production. âž” O\n",
      "\n",
      "Albert     âž” B-PER\n",
      "Einstein,  âž” I-PER\n",
      "a          âž” O\n",
      "renowned   âž” O\n",
      "physicist, âž” O\n",
      "developed  âž” O\n",
      "the        âž” O\n",
      "theory     âž” O\n",
      "of         âž” O\n",
      "relativity. âž” O\n",
      "\n",
      "The        âž” O\n",
      "Nile       âž” B-LOC\n",
      "River      âž” I-LOC\n",
      "is         âž” O\n",
      "the        âž” O\n",
      "longest    âž” O\n",
      "river      âž” O\n",
      "in         âž” O\n",
      "Africa,    âž” B-LOC\n",
      "flowing    âž” O\n",
      "through    âž” O\n",
      "multiple   âž” O\n",
      "countries. âž” O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "    'Paris, the capital of France, is known for its iconic Eiffel Tower.',\n",
    "    'Elon Musk is the CEO of SpaceX and Tesla.',\n",
    "    'The Great Barrier Reef is the world\\'s largest coral reef system, located in Australia.',\n",
    "    'Mount Everest, the highest peak in the world, is part of the Himalayan mountain range.',\n",
    "    'The Mona Lisa, painted by Leonardo da Vinci, is displayed in the Louvre Museum in Paris.',\n",
    "    'Barack Obama served as the 44th President of the United States from 2009 to 2017.',\n",
    "    'The Amazon rainforest, often called the \"lungs of the Earth,\" is vital for global oxygen production.',\n",
    "    'Albert Einstein, a renowned physicist, developed the theory of relativity.',\n",
    "    'The Nile River is the longest river in Africa, flowing through multiple countries.',\n",
    "]\n",
    "\n",
    "\n",
    "for e in example_sentences:\n",
    "    words = e.split()    # Note: assumes white-space tokenization is OK\n",
    "    ner_tags = predict_ner(words)\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        print(f'{word:10s} âž” {tag}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "To get a better intuitive understanding of tagging results, let's implement a visualization using the[`displacy`](https://explosion.ai/demos/displacy-ent) library.\n",
    "\n",
    "The code here mostly maps the IOB tags to character offets and formats the data for displacy. Unless you're interested in modifying this or otherwise working with this library, there's no need to go through this in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paris, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "the capital of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    France, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "is known for its iconic \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Eiffel Tower.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Musk \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "is the CEO of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SpaceX \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Organization</span>\n",
       "</mark>\n",
       "and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Organization</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Barrier Reef \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "is the world's largest coral reef system, located in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Australia.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mount Everest, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "the highest peak in the world, is part of the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Himalayan \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "mountain range.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mona Lisa, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "painted by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Leonardo da Vinci, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "is displayed in the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Louvre Museum \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paris.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "served as the 44th President of the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "from 2009 to 2017.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon rainforest, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "often called the &quot;lungs of the Earth,&quot; is vital for global oxygen production.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Albert Einstein, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "a renowned physicist, developed the theory of relativity.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nile River \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "is the longest river in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Africa, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "flowing through multiple countries.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "# Mapping of MultiNERD types for displacy\n",
    "\n",
    "type_map = {\n",
    "    'PER': 'Person',\n",
    "    'LOC': 'Location',\n",
    "    'ORG': 'Organization',\n",
    "    'ANIM': 'Animal',\n",
    "    'BIO': 'Biological entity',\n",
    "    'CEL': 'Celestial Body',\n",
    "    'DIS': 'Disease',\n",
    "    'EVE': 'Event',\n",
    "    'FOOD': 'Food',\n",
    "    'INST': 'Instrument',\n",
    "    'MEDIA': 'Media',\n",
    "    'PLANT': 'Plant',\n",
    "    'MYTH': 'Mythological entity',\n",
    "    'TIME': 'Time',\n",
    "    'VEHI': 'Vehicle',\n",
    "}\n",
    "\n",
    "def render_with_displacy(words, tags):\n",
    "    tagged, offset, start, label = [], 0, None, None\n",
    "    for word, tag in zip(words, tags):\n",
    "        if tag[0] in 'OB' and start is not None:    # current ends\n",
    "            tagged.append({\n",
    "                'start': start,\n",
    "                'end': offset,\n",
    "                'label': type_map.get(label, label)\n",
    "            })\n",
    "            start, label = None, None\n",
    "        if tag[0] == 'B':\n",
    "            start, label = offset, tag[2:]\n",
    "        elif tag[0] == 'I':\n",
    "            if start is None:    # I without B, but nevermind\n",
    "                start, label = offset, tag[2:]\n",
    "        else:\n",
    "            assert tag == 'O', 'unexpected tag {}'.format(tag)\n",
    "        offset += len(word) + 1    # +1 for space\n",
    "    if start:    # span open at sentence end\n",
    "        tagged.append({\n",
    "                'start': start,\n",
    "                'end': offset,\n",
    "                'label': type_map.get(label, label)\n",
    "        })\n",
    "    doc = {\n",
    "        'text': ' '.join(words),\n",
    "        'ents': tagged\n",
    "    }\n",
    "    displacy.render(doc, style='ent', jupyter=True, manual=True)\n",
    "\n",
    "\n",
    "for e in example_sentences:\n",
    "    words = e.split()    # Note: assumes white-space tokenization is OK\n",
    "    ner_tags = predict_ner(words)\n",
    "    render_with_displacy(words, ner_tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Method to Apply to Validation Dataset (& Then Apply it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to('cpu')\n",
    "    attention_mask = batch[\"attention_mask\"].to('cpu')\n",
    "    labels = batch[\"labels\"].to('cpu')\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = trainer.model(input_ids, \n",
    "                               attention_mask\n",
    "                               )\n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        predicted_label = torch.argmax(output.logits, \n",
    "                                       axis=-1\n",
    "                                       ).cpu().numpy()\n",
    "        \n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, 31), \n",
    "                         labels.view(-1), \n",
    "                         reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Above Function to Entire Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32820/32820 [24:15<00:00, 22.55 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "eval_set = encoded_ds['test']\n",
    "\n",
    "eval_set = eval_set.map(forward_pass_with_label,\n",
    "                        batched=True,\n",
    "                        batch_size=32)\n",
    "\n",
    "eval_df = eval_set.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Up Padding Tokens\n",
    "\n",
    "Defined a placeholder label ID for special tokens (e.g. `[IGN]`) and tokens that represent continuation wordpieces. For example, if the word `Partition` is tokenized into the parts `Part` and `##ition`, the subword token `##ition` would get this ID.\n",
    "\n",
    "(Here the \"magic\" value -100 is significant: this matches the default pythorch `ignore_index`, a value that is ignored in loss functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6464, 18, 239, 20, 339, 621, 1273, 21, 18, 19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[O, O, O, O, B-EVE, I-EVE, I-EVE, O, O, O, O, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-O...</td>\n",
       "      <td>[â–Between, â–the, â–end, â–of, â–World, â–War, â–II,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [6464, 18, 239, 20, 339, 621, 1273, 21, 18, 19...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [O, O, O, O, B-EVE, I-EVE, I-EVE, O, O, O, O, ...   \n",
       "\n",
       "                                                loss  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                     predicted_label  \\\n",
       "0  [B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-O...   \n",
       "\n",
       "                                        input_tokens  \n",
       "0  [â–Between, â–the, â–end, â–of, â–World, â–War, â–II,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[-100] = \"IGN\"\n",
    "eval_df[\"input_tokens\"] = eval_df[\"input_ids\"].apply(\n",
    "    lambda x: tokenizer.convert_ids_to_tokens(x))\n",
    "eval_df[\"predicted_label\"] = eval_df[\"predicted_label\"].apply(\n",
    "    lambda x: [id2label[i] for i in x])\n",
    "eval_df[\"labels\"] = eval_df[\"labels\"].apply(\n",
    "    lambda x: [id2label[i] for i in x])\n",
    "eval_df['loss'] = eval_df.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "eval_df['predicted_label'] = eval_df.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "eval_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unwrap Each Token Within Sample Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6464</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>â–Between</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>â–the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>â–end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>â–of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B-EVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>â–World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I-EVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>â–War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1273</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I-EVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>â–II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids token_type_ids attention_mask labels  loss predicted_label  \\\n",
       "0      6464              0              1      O   0.0           B-ORG   \n",
       "0        18              0              1      O   0.0           B-ORG   \n",
       "0       239              0              1      O   0.0           B-ORG   \n",
       "0        20              0              1      O   0.0           B-ORG   \n",
       "0       339              0              1  B-EVE   0.0           B-ORG   \n",
       "0       621              0              1  I-EVE   0.0           B-ORG   \n",
       "0      1273              0              1  I-EVE   0.0           B-ORG   \n",
       "\n",
       "  input_tokens  \n",
       "0     â–Between  \n",
       "0         â–the  \n",
       "0         â–end  \n",
       "0          â–of  \n",
       "0       â–World  \n",
       "0         â–War  \n",
       "0          â–II  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df_tokens = eval_df.apply(pd.Series.explode)\n",
    "eval_df_tokens = eval_df_tokens.query(\"labels != 'IGN'\")\n",
    "eval_df_tokens[\"loss\"] = eval_df_tokens[\"loss\"].astype(float).round(2)\n",
    "eval_df_tokens.head(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Which Tokens Have Accumulated Most Loss in Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_tokens</th>\n",
       "      <td>â–</td>\n",
       "      <td>â–and</td>\n",
       "      <td>â–the</td>\n",
       "      <td>â–a</td>\n",
       "      <td>â–of</td>\n",
       "      <td>â–to</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>â–in</td>\n",
       "      <td>â–with</td>\n",
       "      <td>â–or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>121976</td>\n",
       "      <td>20850</td>\n",
       "      <td>35218</td>\n",
       "      <td>13900</td>\n",
       "      <td>22056</td>\n",
       "      <td>12762</td>\n",
       "      <td>38</td>\n",
       "      <td>16374</td>\n",
       "      <td>6058</td>\n",
       "      <td>3416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>3.528</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>4011.37</td>\n",
       "      <td>275.17</td>\n",
       "      <td>257.03</td>\n",
       "      <td>186.62</td>\n",
       "      <td>170.33</td>\n",
       "      <td>142.56</td>\n",
       "      <td>134.08</td>\n",
       "      <td>121.77</td>\n",
       "      <td>119.1</td>\n",
       "      <td>118.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1       2       3       4       5       6       7  \\\n",
       "input_tokens        â–    â–and    â–the      â–a     â–of     â–to     â–Qu     â–in   \n",
       "count          121976   20850   35218   13900   22056   12762      38   16374   \n",
       "mean            0.033   0.013   0.007   0.013   0.008   0.011   3.528   0.007   \n",
       "sum           4011.37  275.17  257.03  186.62  170.33  142.56  134.08  121.77   \n",
       "\n",
       "                  8       9  \n",
       "input_tokens  â–with     â–or  \n",
       "count          6058    3416  \n",
       "mean           0.02   0.035  \n",
       "sum           119.1  118.66  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    eval_df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Which Label IDs Have Most Loss in Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>B-BIO</td>\n",
       "      <td>I-TIME</td>\n",
       "      <td>B-VEHI</td>\n",
       "      <td>I-VEHI</td>\n",
       "      <td>I-DIS</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-FOOD</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>...</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-MEDIA</td>\n",
       "      <td>I-CEL</td>\n",
       "      <td>I-BIO</td>\n",
       "      <td>I-INST</td>\n",
       "      <td>I-MYTH</td>\n",
       "      <td>B-EVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>830</td>\n",
       "      <td>1154</td>\n",
       "      <td>28</td>\n",
       "      <td>508</td>\n",
       "      <td>160</td>\n",
       "      <td>174</td>\n",
       "      <td>2502</td>\n",
       "      <td>2268</td>\n",
       "      <td>1656</td>\n",
       "      <td>2360</td>\n",
       "      <td>...</td>\n",
       "      <td>7824</td>\n",
       "      <td>15984</td>\n",
       "      <td>15700</td>\n",
       "      <td>15014</td>\n",
       "      <td>2818</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>18</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>506.44</td>\n",
       "      <td>479.84</td>\n",
       "      <td>8.18</td>\n",
       "      <td>75.45</td>\n",
       "      <td>21.98</td>\n",
       "      <td>20.44</td>\n",
       "      <td>283.74</td>\n",
       "      <td>220.75</td>\n",
       "      <td>153.39</td>\n",
       "      <td>185.36</td>\n",
       "      <td>...</td>\n",
       "      <td>11.41</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.38</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1      2       3       4       5       6       7   \\\n",
       "labels  I-PLANT  I-ANIM  B-BIO  I-TIME  B-VEHI  I-VEHI   I-DIS  B-ANIM   \n",
       "count       830    1154     28     508     160     174    2502    2268   \n",
       "mean       0.61   0.416  0.292   0.149   0.137   0.117   0.113   0.097   \n",
       "sum      506.44  479.84   8.18   75.45   21.98   20.44  283.74  220.75   \n",
       "\n",
       "            8        9   ...     21     22     23     24       25     26  \\\n",
       "labels  I-FOOD  B-PLANT  ...  I-ORG  I-PER  B-LOC  B-PER  I-MEDIA  I-CEL   \n",
       "count     1656     2360  ...   7824  15984  15700  15014     2818     38   \n",
       "mean     0.093    0.079  ...  0.001    0.0    0.0    0.0      0.0    0.0   \n",
       "sum     153.39   185.36  ...  11.41   5.82   5.38   1.24     0.02    0.0   \n",
       "\n",
       "           27      28      29     30  \n",
       "labels  I-BIO  I-INST  I-MYTH  B-EVE  \n",
       "count       8      44      18    598  \n",
       "mean      0.0     0.0     0.0    0.0  \n",
       "sum       0.0     0.0     0.0    0.0  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    eval_df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    "    .fillna(0)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Function to Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "\n",
    "    # Set tick label font size\n",
    "    plt.tick_params(axis='both', which='both', labelsize=10)\n",
    "\n",
    "    # Value name for rotating horizontally\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Set title\n",
    "    plt.title(\"Normalized Confusion Matrix of XLNET-NER-SystemA\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_df_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m eval_token_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(eval_df_tokens[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m eval_df_tokens \u001b[39m=\u001b[39m eval_df_tokens[eval_df_tokens[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mB-PER\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m eval_df_tokens \u001b[39m=\u001b[39m eval_df_tokens[eval_df_tokens[\u001b[39m'\u001b[39m\u001b[39mpredicted_label\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mB-PER\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_df_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "eval_token_list = list(set(eval_df_tokens['labels']))\n",
    "eval_df_tokens = eval_df_tokens[eval_df_tokens['labels'] != 'B-PER']\n",
    "eval_df_tokens = eval_df_tokens[eval_df_tokens['predicted_label'] != 'B-PER']\n",
    "plot_confusion_matrix(eval_df_tokens[\"labels\"], eval_df_tokens[\"predicted_label\"],\n",
    "                      eval_token_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define & Call Function to Display Example Token Sequences Along With Labels & Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>â–species</td>\n",
       "      <td>â–include</td>\n",
       "      <td>â–Coast</td>\n",
       "      <td>â–Live</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–a</td>\n",
       "      <td>gri</td>\n",
       "      <td>folia</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–Engel</td>\n",
       "      <td>mann</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–</td>\n",
       "      <td>eng</td>\n",
       "      <td>elman</td>\n",
       "      <td>ni</td>\n",
       "      <td>i</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–Canyon</td>\n",
       "      <td>â–Live</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–</td>\n",
       "      <td>ch</td>\n",
       "      <td>ry</td>\n",
       "      <td>s</td>\n",
       "      <td>ole</td>\n",
       "      <td>pi</td>\n",
       "      <td>s</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–and</td>\n",
       "      <td>â–Baja</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–peninsula</td>\n",
       "      <td>ris</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;sep&gt;</td>\n",
       "      <td>&lt;cls&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.72</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.78</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1        2        3        4        5     6        7   \\\n",
       "tokens  â–species  â–include   â–Coast    â–Live     â–Oak        â–     (        â–   \n",
       "labels         O         O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds          O         O        O        O        O        O     O  B-PLANT   \n",
       "losses      0.00      0.00     0.00     0.00     0.04     0.00  0.00     0.27   \n",
       "\n",
       "             8        9     10    11       12    13       14       15  \\\n",
       "tokens        \"      â–Qu    er   cus       â–a   gri    folia        â–   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN  I-PLANT   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.39     0.20  8.40  0.00     8.17  0.00     7.24     0.00   \n",
       "\n",
       "             16       17       18       19    20      21    22      23  \\\n",
       "tokens        \"        â–        )        â–     ,  â–Engel  mann    â–Oak   \n",
       "labels      IGN  I-PLANT      IGN        O   IGN  B-ANIM   IGN  I-ANIM   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT     O       O     O       O   \n",
       "losses     0.00     0.06     0.00     0.00  7.99    0.00  8.56    0.00   \n",
       "\n",
       "            24    25       26       27       28    29    30      31    32  \\\n",
       "tokens       â–     (        â–        \"      â–Qu    er   cus       â–   eng   \n",
       "labels  I-ANIM   IGN   I-ANIM      IGN   I-ANIM   IGN   IGN  I-ANIM   IGN   \n",
       "preds        O     O  B-PLANT  I-PLANT  I-PLANT     O     O       O     O   \n",
       "losses    0.00  0.00     4.74     0.00     4.72  9.20  0.00    8.91  0.00   \n",
       "\n",
       "             33       34       35       36       37       38       39  \\\n",
       "tokens    elman       ni        i        â–        \"        â–        )   \n",
       "labels      IGN      IGN      IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses     9.26     0.00     0.00     3.73     0.00     0.00     0.00   \n",
       "\n",
       "             40    41       42       43       44       45    46       47  \\\n",
       "tokens        â–     ,  â–Canyon    â–Live     â–Oak        â–     (        â–   \n",
       "labels        O   IGN  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds   I-PLANT     O        O        O        O        O     O  B-PLANT   \n",
       "losses     0.00  9.03     0.00     9.64     0.00     0.00  0.00     0.15   \n",
       "\n",
       "             48       49    50    51       52    53       54       55  \\\n",
       "tokens        \"      â–Qu    er   cus        â–    ch       ry        s   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN      IGN   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.27     0.17  8.33  0.00     8.14  0.00     7.41     0.00   \n",
       "\n",
       "             56       57       58       59       60       61       62    63  \\\n",
       "tokens      ole       pi        s        â–        \"        â–        )     â–   \n",
       "labels      IGN      IGN      IGN  I-PLANT      IGN  I-PLANT      IGN     O   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT     O   \n",
       "losses     0.00     0.10     0.00     0.00     0.00     0.00     0.00  0.00   \n",
       "\n",
       "          64    65      66      67      68    69      70       71       72  \\\n",
       "tokens     ,  â–and   â–Baja    â–Oak       â–     (       â–        \"      â–Qu   \n",
       "labels   IGN     O  B-ANIM  I-ANIM  I-ANIM   IGN  I-ANIM      IGN   I-ANIM   \n",
       "preds      O     O       O       O       O     O       O  B-PLANT  I-PLANT   \n",
       "losses  7.54  0.00    8.14    0.00    0.00  0.00    0.00     4.41     4.78   \n",
       "\n",
       "          73    74          75    76       77       78       79       80  \\\n",
       "tokens    er   cus  â–peninsula   ris        â–        \"        â–        )   \n",
       "labels   IGN   IGN      I-ANIM   IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds      O     O           O     O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses  9.20  0.00        8.67  0.00     9.59     0.00     0.00     4.23   \n",
       "\n",
       "             81    82     83     84  \n",
       "tokens        â–     .  <sep>  <cls>  \n",
       "labels        O   IGN    IGN    IGN  \n",
       "preds   I-PLANT     O      O      O  \n",
       "losses     0.00  8.83   0.00   9.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>â–species</td>\n",
       "      <td>â–include</td>\n",
       "      <td>â–Coast</td>\n",
       "      <td>â–Live</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–a</td>\n",
       "      <td>gri</td>\n",
       "      <td>folia</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–Engel</td>\n",
       "      <td>mann</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–</td>\n",
       "      <td>eng</td>\n",
       "      <td>elman</td>\n",
       "      <td>ni</td>\n",
       "      <td>i</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–Canyon</td>\n",
       "      <td>â–Live</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–</td>\n",
       "      <td>ch</td>\n",
       "      <td>ry</td>\n",
       "      <td>s</td>\n",
       "      <td>ole</td>\n",
       "      <td>pi</td>\n",
       "      <td>s</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–and</td>\n",
       "      <td>â–Baja</td>\n",
       "      <td>â–Oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–peninsula</td>\n",
       "      <td>ris</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;sep&gt;</td>\n",
       "      <td>&lt;cls&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.72</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.78</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1        2        3        4        5     6        7   \\\n",
       "tokens  â–species  â–include   â–Coast    â–Live     â–Oak        â–     (        â–   \n",
       "labels         O         O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds          O         O        O        O        O        O     O  B-PLANT   \n",
       "losses      0.00      0.00     0.00     0.00     0.04     0.00  0.00     0.27   \n",
       "\n",
       "             8        9     10    11       12    13       14       15  \\\n",
       "tokens        \"      â–Qu    er   cus       â–a   gri    folia        â–   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN  I-PLANT   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.39     0.20  8.40  0.00     8.17  0.00     7.24     0.00   \n",
       "\n",
       "             16       17       18       19    20      21    22      23  \\\n",
       "tokens        \"        â–        )        â–     ,  â–Engel  mann    â–Oak   \n",
       "labels      IGN  I-PLANT      IGN        O   IGN  B-ANIM   IGN  I-ANIM   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT     O       O     O       O   \n",
       "losses     0.00     0.06     0.00     0.00  7.99    0.00  8.56    0.00   \n",
       "\n",
       "            24    25       26       27       28    29    30      31    32  \\\n",
       "tokens       â–     (        â–        \"      â–Qu    er   cus       â–   eng   \n",
       "labels  I-ANIM   IGN   I-ANIM      IGN   I-ANIM   IGN   IGN  I-ANIM   IGN   \n",
       "preds        O     O  B-PLANT  I-PLANT  I-PLANT     O     O       O     O   \n",
       "losses    0.00  0.00     4.74     0.00     4.72  9.20  0.00    8.91  0.00   \n",
       "\n",
       "             33       34       35       36       37       38       39  \\\n",
       "tokens    elman       ni        i        â–        \"        â–        )   \n",
       "labels      IGN      IGN      IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses     9.26     0.00     0.00     3.73     0.00     0.00     0.00   \n",
       "\n",
       "             40    41       42       43       44       45    46       47  \\\n",
       "tokens        â–     ,  â–Canyon    â–Live     â–Oak        â–     (        â–   \n",
       "labels        O   IGN  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds   I-PLANT     O        O        O        O        O     O  B-PLANT   \n",
       "losses     0.00  9.03     0.00     9.64     0.00     0.00  0.00     0.15   \n",
       "\n",
       "             48       49    50    51       52    53       54       55  \\\n",
       "tokens        \"      â–Qu    er   cus        â–    ch       ry        s   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN      IGN   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.27     0.17  8.33  0.00     8.14  0.00     7.41     0.00   \n",
       "\n",
       "             56       57       58       59       60       61       62    63  \\\n",
       "tokens      ole       pi        s        â–        \"        â–        )     â–   \n",
       "labels      IGN      IGN      IGN  I-PLANT      IGN  I-PLANT      IGN     O   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT     O   \n",
       "losses     0.00     0.10     0.00     0.00     0.00     0.00     0.00  0.00   \n",
       "\n",
       "          64    65      66      67      68    69      70       71       72  \\\n",
       "tokens     ,  â–and   â–Baja    â–Oak       â–     (       â–        \"      â–Qu   \n",
       "labels   IGN     O  B-ANIM  I-ANIM  I-ANIM   IGN  I-ANIM      IGN   I-ANIM   \n",
       "preds      O     O       O       O       O     O       O  B-PLANT  I-PLANT   \n",
       "losses  7.54  0.00    8.14    0.00    0.00  0.00    0.00     4.41     4.78   \n",
       "\n",
       "          73    74          75    76       77       78       79       80  \\\n",
       "tokens    er   cus  â–peninsula   ris        â–        \"        â–        )   \n",
       "labels   IGN   IGN      I-ANIM   IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds      O     O           O     O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses  9.20  0.00        8.67  0.00     9.59     0.00     0.00     4.23   \n",
       "\n",
       "             81    82     83     84  \n",
       "tokens        â–     .  <sep>  <cls>  \n",
       "labels        O   IGN    IGN    IGN  \n",
       "preds   I-PLANT     O      O      O  \n",
       "losses     0.00  8.83   0.00   9.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>â–of</td>\n",
       "      <td>â–the</td>\n",
       "      <td>â–chap</td>\n",
       "      <td>ar</td>\n",
       "      <td>ral</td>\n",
       "      <td>â–woodland</td>\n",
       "      <td>â–species</td>\n",
       "      <td>â–include</td>\n",
       "      <td>â–</td>\n",
       "      <td>:</td>\n",
       "      <td>â–canyon</td>\n",
       "      <td>â–live</td>\n",
       "      <td>â–oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–</td>\n",
       "      <td>ch</td>\n",
       "      <td>ry</td>\n",
       "      <td>s</td>\n",
       "      <td>ole</td>\n",
       "      <td>pi</td>\n",
       "      <td>s</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–valley</td>\n",
       "      <td>â–oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–</td>\n",
       "      <td>lob</td>\n",
       "      <td>ata</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–blue</td>\n",
       "      <td>â–oak</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>â–do</td>\n",
       "      <td>ug</td>\n",
       "      <td>las</td>\n",
       "      <td>ii</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>,</td>\n",
       "      <td>â–and</td>\n",
       "      <td>â–gray</td>\n",
       "      <td>â–pine</td>\n",
       "      <td>â–</td>\n",
       "      <td>(</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–Pin</td>\n",
       "      <td>us</td>\n",
       "      <td>â–</td>\n",
       "      <td>sa</td>\n",
       "      <td>bin</td>\n",
       "      <td>iana</td>\n",
       "      <td>â–</td>\n",
       "      <td>\"</td>\n",
       "      <td>â–</td>\n",
       "      <td>)</td>\n",
       "      <td>â–</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;sep&gt;</td>\n",
       "      <td>&lt;cls&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.80</td>\n",
       "      <td>11.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.52</td>\n",
       "      <td>4.32</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.01</td>\n",
       "      <td>5.80</td>\n",
       "      <td>10.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2     3     4          5         6         7     8   \\\n",
       "tokens   â–of  â–the  â–chap    ar   ral  â–woodland  â–species  â–include     â–   \n",
       "labels     O     O      O   IGN   IGN          O         O         O     O   \n",
       "preds      O     O      O     O     O          O         O         O     O   \n",
       "losses  0.00  0.00   0.00  0.00  0.00       0.00      0.00      0.00  0.00   \n",
       "\n",
       "          9        10       11       12       13    14       15    16  \\\n",
       "tokens     :  â–canyon    â–live     â–oak        â–     (        â–     \"   \n",
       "labels   IGN  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   IGN   \n",
       "preds      O        O        O        O        O     O        O     O   \n",
       "losses  0.00     3.04     4.02     3.80    11.31  0.00    11.28  0.00   \n",
       "\n",
       "             17       18       19       20       21       22       23  \\\n",
       "tokens      â–Qu       er      cus        â–       ch       ry        s   \n",
       "labels  I-PLANT      IGN      IGN  I-PLANT      IGN      IGN      IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses     9.23     0.00     0.00     0.04     0.00     0.00     0.00   \n",
       "\n",
       "             24       25    26       27    28       29    30    31    32  \\\n",
       "tokens      ole       pi     s        â–     \"        â–     )     â–     ,   \n",
       "labels      IGN      IGN   IGN  I-PLANT   IGN  I-PLANT   IGN     O   IGN   \n",
       "preds   I-PLANT  I-PLANT     O        O     O        O     O     O     O   \n",
       "losses     0.00     0.00  0.00     9.79  0.00    11.01  0.00  0.00  0.00   \n",
       "\n",
       "             33       34       35    36       37    38       39       40  \\\n",
       "tokens  â–valley     â–oak        â–     (        â–     \"      â–Qu       er   \n",
       "labels  B-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   IGN  I-PLANT      IGN   \n",
       "preds         O        O        O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     3.52     4.32    10.81  0.00    11.45  0.00     9.31     0.00   \n",
       "\n",
       "             41       42       43       44       45    46       47    48  \\\n",
       "tokens      cus        â–      lob      ata        â–     \"        â–     )   \n",
       "labels      IGN  I-PLANT      IGN      IGN  I-PLANT   IGN  I-PLANT   IGN   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT        O     O        O     O   \n",
       "losses     0.00     0.05     0.00     0.00     9.94  0.00    11.00  0.00   \n",
       "\n",
       "          49    50      51      52      53    54      55    56       57  \\\n",
       "tokens     â–     ,   â–blue    â–oak       â–     (       â–     \"      â–Qu   \n",
       "labels     O   IGN  B-ANIM  I-ANIM  I-ANIM   IGN  I-ANIM   IGN   I-ANIM   \n",
       "preds      O     O       O       O       O     O       O     O  B-PLANT   \n",
       "losses  0.00  0.00    4.01    5.80   10.35  0.00   10.90  0.00    10.83   \n",
       "\n",
       "             58       59       60       61       62       63      64    65  \\\n",
       "tokens       er      cus      â–do       ug      las       ii       â–     \"   \n",
       "labels      IGN      IGN   I-ANIM      IGN      IGN      IGN  I-ANIM   IGN   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT       O     O   \n",
       "losses     0.00     0.00     3.68     0.00     0.00     0.00    9.84  0.00   \n",
       "\n",
       "            66    67    68    69    70     71     72    73    74    75    76  \\\n",
       "tokens       â–     )     â–     ,  â–and  â–gray  â–pine     â–     (     â–     \"   \n",
       "labels  I-ANIM   IGN     O   IGN     O      O      O     O   IGN     O   IGN   \n",
       "preds        O     O     O     O     O      O      O     O     O     O     O   \n",
       "losses   10.85  0.00  0.00  0.00  0.00   0.15   0.12  0.00  0.00  0.00  0.00   \n",
       "\n",
       "             77       78       79       80       81       82    83    84  \\\n",
       "tokens     â–Pin       us        â–       sa      bin     iana     â–     \"   \n",
       "labels        O      IGN        O      IGN      IGN      IGN     O   IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT     O     O   \n",
       "losses     4.81     0.00     4.33     0.00     0.00     0.00  0.00  0.00   \n",
       "\n",
       "          85    86    87    88     89     90  \n",
       "tokens     â–     )     â–     .  <sep>  <cls>  \n",
       "labels     O   IGN     O   IGN    IGN    IGN  \n",
       "preds      O     O     O     O      O      O  \n",
       "losses  0.00  0.00  0.00  0.00   0.00   0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        eval_df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n",
    "                               \"preds\": preds, \"losses\": losses}).T\n",
    "        yield eval_df_tmp\n",
    "\n",
    "eval_df[\"total_loss\"] = eval_df[\"loss\"].apply(sum)\n",
    "eval_df_tmp = eval_df.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "for sample in get_samples(eval_df_tmp):\n",
    "    display(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
