{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition Task SystemA (MultiNERD Dataset)\n",
    "\n",
    "### XLNET Base Cased\n",
    "\n",
    "[MultiNERD Dataset] 🤗: https://huggingface.co/datasets/Babelscape/multinerd \n",
    "\n",
    "[xlnet Base Cased Model] 🤗: https://huggingface.co/xlnet-base-cased"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries\n",
    "\n",
    "* [AutoTokenizer](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer): A tokenizer class designed to accommodate the tokenization conventions of various pre-trained models.\n",
    "\n",
    "* [AutoModelForTokenClassification](https://huggingface.co/transformers/model_doc/auto.html#automodelfortokenclassification): An extension of the [AutoModel](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel) class, capable of loading diverse pre-trained models. It supports fine-tuning for the classification of each token within a sequence.\n",
    "\n",
    "* [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments): A straightforward class tailored for storing hyperparameters and other settings essential for model training.\n",
    "\n",
    "* [Trainer](https://huggingface.co/transformers/main_classes/trainer.html): A versatile class that facilitates various forms of training for transformer models.\n",
    "\n",
    "* [DataCollatorForTokenClassification](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py): A class designed for padding token classification examples to the same length during training.\n",
    "\n",
    "* [load_dataset](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset): A function crafted for effortlessly loading datasets, including those from the [datasets](https://huggingface.co/datasets) collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting English Subset of Dataset as input of SystemA\n",
    "\n",
    "* First load the dataset with [`load_dataset`](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset), Then Filter out the non-English examples of the dataset as the training dataset of the SystemA.\n",
    "\n",
    "* System A supposed to be a fine-tuned XLNET model on the English subset of the training set.\n",
    "\n",
    "* The loaded dataset is a dictionary-like container for [`Dataset`](https://huggingface.co/docs/datasets/exploring.html) objects for training, development (validation), and test data. We'll here be using the `tokens` and `ner_tags` fields and ignoring `lang` (language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 20/20 [00:04<00:00,  4.68it/s]\n",
      "Resolving data files: 100%|██████████| 20/20 [00:00<00:00, 144382.24it/s]\n",
      "Resolving data files: 100%|██████████| 20/20 [00:00<00:00, 182361.04it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_dataset = load_dataset('Babelscape/multinerd')\n",
    "data = original_dataset.filter(lambda example: example['lang'] == 'en')\n",
    "data = data.remove_columns([\"lang\"])\n",
    "label_list = ['O',\n",
    " 'B-EVE',\n",
    " 'I-EVE',\n",
    " 'B-LOC',\n",
    " 'B-MEDIA',\n",
    " 'I-MEDIA',\n",
    " 'I-PER',\n",
    " 'B-PER',\n",
    " 'B-DIS',\n",
    " 'I-LOC',\n",
    " 'B-PLANT',\n",
    " 'B-FOOD',\n",
    " 'B-VEHI',\n",
    " 'I-VEHI',\n",
    " 'I-ORG',\n",
    " 'I-FOOD',\n",
    " 'B-ORG',\n",
    " 'B-TIME',\n",
    " 'B-ANIM',\n",
    " 'B-CEL',\n",
    " 'I-TIME',\n",
    " 'B-MYTH',\n",
    " 'I-MYTH',\n",
    " 'I-DIS',\n",
    " 'I-ANIM',\n",
    " 'B-INST',\n",
    " 'I-PLANT',\n",
    " 'B-BIO',\n",
    " 'I-INST',\n",
    " 'I-CEL',\n",
    " 'I-BIO']\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (262560, 2)\n",
      "Validation data shape: (32908, 2)\n",
      "Testing data shape: (32820, 2)\n"
     ]
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    'train': data['train'], \n",
    "    'eval': data['validation'], \n",
    "    'test': data['test']})\n",
    "\n",
    "print('Training data shape:', ds['train'].shape)\n",
    "print('Validation data shape:', ds['eval'].shape)\n",
    "print('Testing data shape:', ds['test'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['The',\n",
       "  'wild',\n",
       "  'bulb',\n",
       "  'vernal',\n",
       "  'squill',\n",
       "  'is',\n",
       "  'known',\n",
       "  'locally',\n",
       "  'as',\n",
       "  '\"',\n",
       "  'grice',\n",
       "  \"'s\",\n",
       "  'onions',\n",
       "  '\"',\n",
       "  'because',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'favourite',\n",
       "  'food',\n",
       "  'of',\n",
       "  'the',\n",
       "  'swine',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  25,\n",
       "  26,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds['train'][12]\n",
    "example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Feature Information About Each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: \n",
      "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "\n",
      "ner_tags: \n",
      "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in ds[\"train\"].features.items():\n",
    "    print(f\"{k}: \\n{v}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Tag Values & Conversions Between String & Integer Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tag values: \n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-ANIM', 'I-ANIM', 'B-BIO', 'I-BIO', 'B-CEL', 'I-CEL', 'B-DIS', 'I-DIS', 'B-EVE', 'I-EVE', 'B-FOOD', 'I-FOOD', 'B-INST', 'I-INST', 'B-MEDIA', 'I-MEDIA', 'B-MYTH', 'I-MYTH', 'B-PLANT', 'I-PLANT', 'B-TIME', 'I-TIME', 'B-VEHI', 'I-VEHI']\n",
      "Number of NER Tags: \n",
      "31\n",
      "id2label: \n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-ANIM', 8: 'I-ANIM', 9: 'B-BIO', 10: 'I-BIO', 11: 'B-CEL', 12: 'I-CEL', 13: 'B-DIS', 14: 'I-DIS', 15: 'B-EVE', 16: 'I-EVE', 17: 'B-FOOD', 18: 'I-FOOD', 19: 'B-INST', 20: 'I-INST', 21: 'B-MEDIA', 22: 'I-MEDIA', 23: 'B-MYTH', 24: 'I-MYTH', 25: 'B-PLANT', 26: 'I-PLANT', 27: 'B-TIME', 28: 'I-TIME', 29: 'B-VEHI', 30: 'I-VEHI'}\n",
      "label2id: \n",
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-ANIM': 7, 'I-ANIM': 8, 'B-BIO': 9, 'I-BIO': 10, 'B-CEL': 11, 'I-CEL': 12, 'B-DIS': 13, 'I-DIS': 14, 'B-EVE': 15, 'I-EVE': 16, 'B-FOOD': 17, 'I-FOOD': 18, 'B-INST': 19, 'I-INST': 20, 'B-MEDIA': 21, 'I-MEDIA': 22, 'B-MYTH': 23, 'I-MYTH': 24, 'B-PLANT': 25, 'I-PLANT': 26, 'B-TIME': 27, 'I-TIME': 28, 'B-VEHI': 29, 'I-VEHI': 30}\n"
     ]
    }
   ],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-ANIM\": 7,\n",
    "    \"I-ANIM\": 8,\n",
    "    \"B-BIO\": 9,\n",
    "    \"I-BIO\": 10,\n",
    "    \"B-CEL\": 11,\n",
    "    \"I-CEL\": 12,\n",
    "    \"B-DIS\": 13,\n",
    "    \"I-DIS\": 14,\n",
    "    \"B-EVE\": 15,\n",
    "    \"I-EVE\": 16,\n",
    "    \"B-FOOD\": 17,\n",
    "    \"I-FOOD\": 18,\n",
    "    \"B-INST\": 19,\n",
    "    \"I-INST\": 20,\n",
    "    \"B-MEDIA\": 21,\n",
    "    \"I-MEDIA\": 22,\n",
    "    \"B-MYTH\": 23,\n",
    "    \"I-MYTH\": 24,\n",
    "    \"B-PLANT\": 25,\n",
    "    \"I-PLANT\": 26,\n",
    "    \"B-TIME\": 27,\n",
    "    \"I-TIME\": 28,\n",
    "    \"B-VEHI\": 29,\n",
    "    \"I-VEHI\": 30,\n",
    "  }\n",
    "\n",
    "id2label = {tag: idx for idx, tag in label2id.items()}\n",
    "\n",
    "pos_tag_values = list(label2id.keys())\n",
    "NUM_OF_LABELS = len(pos_tag_values)\n",
    "\n",
    "print(f\"List of tag values: \\n{pos_tag_values}\")\n",
    "print(f\"Number of NER Tags: \\n{NUM_OF_LABELS}\")\n",
    "print(f\"id2label: \\n{id2label}\")\n",
    "print(f\"label2id: \\n{label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text classification tasks discussed earlier, each data example included a string representing an entire sentence. In contrast, the current data is already segmented into words, denoted by the key `tokens` in the `Dataset` object. Interestingly, the tags in the data correspond to these individual words. Let's examine a sample sentence for better clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The      ➔ O\n",
      "type     ➔ O\n",
      "locality ➔ O\n",
      "is       ➔ O\n",
      "Kīlauea  ➔ I-MEDIA\n",
      ".        ➔ O\n"
     ]
    }
   ],
   "source": [
    "train_words = ds['train']['tokens']\n",
    "train_tags = ds['train']['ner_tags']\n",
    "for word, tag in zip(train_words[0], train_tags[0]):\n",
    "    print(f'{word:8s} ➔ {label_list[tag]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aligning labels (tags) with words, especially when they don't align perfectly with the tokenization of the model, introduces significant complexity. This becomes more apparent once we've loaded a tokenizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental Constants and Parameters Configuration\n",
    "\n",
    "Let's establish critical global variables encompassing the designation of the pre-trained model and the essential hyperparameters instrumental for its meticulous fine-tuning:\n",
    "\n",
    "* `MODEL_CKPT`: The nomenclature of a pre-trained model accessible within the [model repository](https://huggingface.co/models).\n",
    "* `REPORTS_TO`: Incorporating visualization tools vital for comprehensive machine learning experimentation, opt for [TensorBoard](https://www.tensorflow.org/tensorboard) in this context.\n",
    "* `LR`, `WEIGHT_DECAY`, `BATCH_SIZE`, `STEPS`, and `NUM_OF_EPOCHS`: Configurable hyperparameters governing the nuanced fine-tuning process of the model. (Experimenting with diverse values is encouraged!)\n",
    "* `OUTPUT_DIR` and `LOG_DIR`: The designated directories for storing the model and its associated parameters.\n",
    "* `DEVICE` : The computational device on which the model training is executed. In this instance, utilize GPU for optimal training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "MODEL_CKPT = \"xlnet-base-cased\"\n",
    "MODEL_NAME = f\"{MODEL_CKPT}-finetuned-MultiNERD-SystemA\"\n",
    "\n",
    "NUM_OF_EPOCHS = 2\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "STRATEGY = \"epoch\"\n",
    "REPORTS_TO = \"tensorboard\"\n",
    "\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR = 2e-5\n",
    "\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "STEPS = 1250\n",
    "\n",
    "OUTPUT_DIR = f'/srv/users/rudxia/Developer_NLP/notebooks/results/Outdir/{MODEL_CKPT}-finetuned-MultiNERD-SystemA'\n",
    "LOG_DIR= f'/srv/users/rudxia/Developer_NLP/notebooks/results/Log/{MODEL_CKPT}-finetuned-MultiNERD-SystemA'\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Tokenize & Align Inputs\n",
    "\n",
    "Ensuring the alignment of labels (tags) with words that might not align perfectly with the model involves loading a suitable tokenizer using [`AutoTokenizer.from_pretrained`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained). Specifically, we opt for a [\"fast\" tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html) since it offers a mapping from tokens to input words, a crucial requirement for Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "def tokenize_and_align_labels(samples):\n",
    "    tokenized_inputs = tokenizer(samples[\"tokens\"], \n",
    "                                      truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for idx, label in enumerate(samples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        prev_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids: # set special tokens to -100\n",
    "            if word_idx is None or word_idx == prev_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            prev_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Above Function to Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/32908 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 32908/32908 [00:05<00:00, 6129.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_ds = ds.map(tokenize_and_align_labels, \n",
    "                    batched=True, \n",
    "                    remove_columns=\n",
    "                        [\n",
    "                            'ner_tags', \n",
    "                            'tokens'\n",
    "                        ]\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = (AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    num_labels=NUM_OF_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    ).to(DEVICE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = pos_tag_values\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "labels = [label_list[i] for i in example[f'ner_tags']]\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, \n",
    "                            axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, \n",
    "                              references=true_labels)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    OUTPUT_DIR,    # output directory for checkpoints and predictions\n",
    "    MODEL_NAME,\n",
    "    log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    "    logging_dir = LOG_DIR,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=NUM_OF_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    report_to=REPORTS_TO,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    save_strategy=STRATEGY,\n",
    "    hub_private_repo=False,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Trainer and Subclass Trainer to Handle Class Imbalance\n",
    "\n",
    "To fine-tune the pre-trained model, we'll instantiate a [`Trainer`](https://huggingface.co/transformers/main_classes/trainer.html) object. This involves providing the pre-trained model, configuration settings, training and validation datasets, and the previously defined evaluation metric.\n",
    "\n",
    "Most of this should be familiar from the sentence classification notebook, with one notable exception: we include a [`DataCollatorForTokenClassification`](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py) with the trainer. This class efficiently handles the padding of token classification examples in batches, ensuring they are of the same length, as required by PyTorch. The tokenizer's `[PAD]` symbol is employed for padding the output, along with the specified `label_pad_token_id` for padding the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, \n",
    "                     model, \n",
    "                     inputs, \n",
    "                     return_outputs=False):\n",
    "        \n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "            [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \n",
    "             9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0,\n",
    "             16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0,\n",
    "             23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0], \n",
    "            device=DEVICE)\n",
    "        )\n",
    "        loss = loss_fct(logits.view(-1, \n",
    "                                    self.model.config.num_labels), \n",
    "                        labels.view(-1)\n",
    "                        )\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "trainer = CustomTrainer(model, \n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer,\n",
    "                  train_dataset=encoded_ds[\"train\"],\n",
    "                  eval_dataset=encoded_ds[\"eval\"],\n",
    "                  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10940' max='10940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10940/10940 1:06:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Anim</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Cel</th>\n",
       "      <th>Dis</th>\n",
       "      <th>Eve</th>\n",
       "      <th>Food</th>\n",
       "      <th>Inst</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Media</th>\n",
       "      <th>Myth</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Plant</th>\n",
       "      <th>Time</th>\n",
       "      <th>Vehi</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.104612</td>\n",
       "      <td>{'precision': 0.5805658056580566, 'recall': 0.8827930174563591, 'f1': 0.7004699480583725, 'number': 3208}</td>\n",
       "      <td>{'precision': 0.5, 'recall': 0.375, 'f1': 0.42857142857142855, 'number': 16}</td>\n",
       "      <td>{'precision': 0.5535714285714286, 'recall': 0.7560975609756098, 'f1': 0.6391752577319588, 'number': 82}</td>\n",
       "      <td>{'precision': 0.5325301204819277, 'recall': 0.8735177865612648, 'f1': 0.6616766467065868, 'number': 1518}</td>\n",
       "      <td>{'precision': 0.9344262295081968, 'recall': 0.9715909090909091, 'f1': 0.9526462395543176, 'number': 704}</td>\n",
       "      <td>{'precision': 0.384297520661157, 'recall': 0.8215547703180212, 'f1': 0.5236486486486486, 'number': 1132}</td>\n",
       "      <td>{'precision': 0.4444444444444444, 'recall': 0.6666666666666666, 'f1': 0.5333333333333333, 'number': 24}</td>\n",
       "      <td>{'precision': 0.9922686840136338, 'recall': 0.9926813040585496, 'f1': 0.9924749511495448, 'number': 24048}</td>\n",
       "      <td>{'precision': 0.9200819672131147, 'recall': 0.980349344978166, 'f1': 0.9492600422832981, 'number': 916}</td>\n",
       "      <td>{'precision': 0.3835616438356164, 'recall': 0.875, 'f1': 0.5333333333333333, 'number': 64}</td>\n",
       "      <td>{'precision': 0.9717548076923077, 'recall': 0.9773345421577516, 'f1': 0.9745366882627693, 'number': 6618}</td>\n",
       "      <td>{'precision': 0.9925558312655087, 'recall': 0.9876543209876543, 'f1': 0.9900990099009901, 'number': 10530}</td>\n",
       "      <td>{'precision': 0.41868317388857623, 'recall': 0.8322147651006712, 'f1': 0.5570947210782479, 'number': 1788}</td>\n",
       "      <td>{'precision': 0.6090425531914894, 'recall': 0.7923875432525952, 'f1': 0.6887218045112782, 'number': 578}</td>\n",
       "      <td>{'precision': 0.6511627906976745, 'recall': 0.875, 'f1': 0.7466666666666667, 'number': 64}</td>\n",
       "      <td>0.863486</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.911922</td>\n",
       "      <td>0.982975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.115915</td>\n",
       "      <td>{'precision': 0.6152164407520769, 'recall': 0.8771820448877805, 'f1': 0.7232074016962221, 'number': 3208}</td>\n",
       "      <td>{'precision': 0.47058823529411764, 'recall': 1.0, 'f1': 0.6399999999999999, 'number': 16}</td>\n",
       "      <td>{'precision': 0.603448275862069, 'recall': 0.8536585365853658, 'f1': 0.7070707070707071, 'number': 82}</td>\n",
       "      <td>{'precision': 0.5658783783783784, 'recall': 0.8827404479578392, 'f1': 0.6896551724137931, 'number': 1518}</td>\n",
       "      <td>{'precision': 0.9301075268817204, 'recall': 0.9829545454545454, 'f1': 0.9558011049723756, 'number': 704}</td>\n",
       "      <td>{'precision': 0.4458413926499033, 'recall': 0.8144876325088339, 'f1': 0.5762499999999999, 'number': 1132}</td>\n",
       "      <td>{'precision': 0.4117647058823529, 'recall': 0.5833333333333334, 'f1': 0.4827586206896552, 'number': 24}</td>\n",
       "      <td>{'precision': 0.9946795244825006, 'recall': 0.9950931470392548, 'f1': 0.9948862927701326, 'number': 24048}</td>\n",
       "      <td>{'precision': 0.9430379746835443, 'recall': 0.9759825327510917, 'f1': 0.9592274678111588, 'number': 916}</td>\n",
       "      <td>{'precision': 0.6666666666666666, 'recall': 0.875, 'f1': 0.7567567567567567, 'number': 64}</td>\n",
       "      <td>{'precision': 0.9779722389861195, 'recall': 0.9794499848896948, 'f1': 0.9787105541295485, 'number': 6618}</td>\n",
       "      <td>{'precision': 0.9929857819905213, 'recall': 0.9948717948717949, 'f1': 0.9939278937381405, 'number': 10530}</td>\n",
       "      <td>{'precision': 0.46262507357268984, 'recall': 0.8791946308724832, 'f1': 0.6062475896644813, 'number': 1788}</td>\n",
       "      <td>{'precision': 0.6657534246575343, 'recall': 0.8408304498269896, 'f1': 0.7431192660550459, 'number': 578}</td>\n",
       "      <td>{'precision': 0.7631578947368421, 'recall': 0.90625, 'f1': 0.8285714285714286, 'number': 64}</td>\n",
       "      <td>0.883212</td>\n",
       "      <td>0.971378</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.985433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = \"/srv/users/rudxia/Developer_NLP/notebooks/results/saved_model/XLNET_MultiNERD_SystemA\"\n",
    "trainer.save_model(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               = 22355475GF\n",
      "  train_loss               =     0.1292\n",
      "  train_runtime            = 1:06:53.31\n",
      "  train_samples_per_second =    130.845\n",
      "  train_steps_per_second   =      2.726\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model is now available as `trainer.model`. Here we define functions to predict tags for a user-defined string.\n",
    "\n",
    "The main complexity here arises from the need to map back from token labels to word labels, inverting the mapping performed in `encode_dataset`. The process is basically the same as in `compute_metrics` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "model.eval()    # switch to evaluation mode\n",
    "model.to('cpu')    # switch to CPU\n",
    "\n",
    "\n",
    "def word_start_tokens(tokenized):\n",
    "    \"\"\"Return list of bool identifying which tokens start words.\"\"\"\n",
    "    prev_word_idx = None\n",
    "    is_word_start = []\n",
    "    for word_idx in tokenized.word_ids():\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            is_word_start.append(False)\n",
    "        else:\n",
    "            is_word_start.append(True)\n",
    "        prev_word_idx = word_idx\n",
    "    return is_word_start\n",
    "\n",
    "\n",
    "def predict_ner(words):\n",
    "    tokenized = tokenizer(words, is_split_into_words=True, return_tensors='pt')\n",
    "    pred = model(**tokenized)\n",
    "    pred_idx = pred.logits.detach().numpy().argmax(axis=2)\n",
    "    token_labels = [label_list[i] for s in pred_idx for i in s]\n",
    "    word_labels = []\n",
    "    for label, is_word_start in zip(token_labels, word_start_tokens(tokenized)):\n",
    "        if is_word_start:\n",
    "            word_labels.append(label)\n",
    "    return word_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that out on a couple of example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris,     ➔ B-LOC\n",
      "the        ➔ O\n",
      "capital    ➔ O\n",
      "of         ➔ O\n",
      "France,    ➔ B-LOC\n",
      "is         ➔ O\n",
      "known      ➔ O\n",
      "for        ➔ O\n",
      "its        ➔ O\n",
      "iconic     ➔ O\n",
      "Eiffel     ➔ B-LOC\n",
      "Tower.     ➔ I-LOC\n",
      "\n",
      "Elon       ➔ B-PER\n",
      "Musk       ➔ I-PER\n",
      "is         ➔ O\n",
      "the        ➔ O\n",
      "CEO        ➔ O\n",
      "of         ➔ O\n",
      "SpaceX     ➔ B-ORG\n",
      "and        ➔ O\n",
      "Tesla.     ➔ B-ORG\n",
      "\n",
      "The        ➔ O\n",
      "Great      ➔ B-LOC\n",
      "Barrier    ➔ I-LOC\n",
      "Reef       ➔ I-LOC\n",
      "is         ➔ O\n",
      "the        ➔ O\n",
      "world's    ➔ O\n",
      "largest    ➔ O\n",
      "coral      ➔ O\n",
      "reef       ➔ O\n",
      "system,    ➔ O\n",
      "located    ➔ O\n",
      "in         ➔ O\n",
      "Australia. ➔ B-LOC\n",
      "\n",
      "Mount      ➔ B-LOC\n",
      "Everest,   ➔ I-LOC\n",
      "the        ➔ O\n",
      "highest    ➔ O\n",
      "peak       ➔ O\n",
      "in         ➔ O\n",
      "the        ➔ O\n",
      "world,     ➔ O\n",
      "is         ➔ O\n",
      "part       ➔ O\n",
      "of         ➔ O\n",
      "the        ➔ O\n",
      "Himalayan  ➔ B-LOC\n",
      "mountain   ➔ O\n",
      "range.     ➔ O\n",
      "\n",
      "The        ➔ O\n",
      "Mona       ➔ B-LOC\n",
      "Lisa,      ➔ I-LOC\n",
      "painted    ➔ O\n",
      "by         ➔ O\n",
      "Leonardo   ➔ B-PER\n",
      "da         ➔ I-PER\n",
      "Vinci,     ➔ I-PER\n",
      "is         ➔ O\n",
      "displayed  ➔ O\n",
      "in         ➔ O\n",
      "the        ➔ O\n",
      "Louvre     ➔ B-LOC\n",
      "Museum     ➔ I-LOC\n",
      "in         ➔ O\n",
      "Paris.     ➔ B-LOC\n",
      "\n",
      "Barack     ➔ B-PER\n",
      "Obama      ➔ I-PER\n",
      "served     ➔ O\n",
      "as         ➔ O\n",
      "the        ➔ O\n",
      "44th       ➔ O\n",
      "President  ➔ O\n",
      "of         ➔ O\n",
      "the        ➔ O\n",
      "United     ➔ B-LOC\n",
      "States     ➔ I-LOC\n",
      "from       ➔ O\n",
      "2009       ➔ O\n",
      "to         ➔ O\n",
      "2017.      ➔ O\n",
      "\n",
      "The        ➔ O\n",
      "Amazon     ➔ B-LOC\n",
      "rainforest, ➔ I-LOC\n",
      "often      ➔ O\n",
      "called     ➔ O\n",
      "the        ➔ O\n",
      "\"lungs     ➔ O\n",
      "of         ➔ O\n",
      "the        ➔ O\n",
      "Earth,\"    ➔ O\n",
      "is         ➔ O\n",
      "vital      ➔ O\n",
      "for        ➔ O\n",
      "global     ➔ O\n",
      "oxygen     ➔ O\n",
      "production. ➔ O\n",
      "\n",
      "Albert     ➔ B-PER\n",
      "Einstein,  ➔ I-PER\n",
      "a          ➔ O\n",
      "renowned   ➔ O\n",
      "physicist, ➔ O\n",
      "developed  ➔ O\n",
      "the        ➔ O\n",
      "theory     ➔ O\n",
      "of         ➔ O\n",
      "relativity. ➔ O\n",
      "\n",
      "The        ➔ O\n",
      "Nile       ➔ B-LOC\n",
      "River      ➔ I-LOC\n",
      "is         ➔ O\n",
      "the        ➔ O\n",
      "longest    ➔ O\n",
      "river      ➔ O\n",
      "in         ➔ O\n",
      "Africa,    ➔ B-LOC\n",
      "flowing    ➔ O\n",
      "through    ➔ O\n",
      "multiple   ➔ O\n",
      "countries. ➔ O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "    'Paris, the capital of France, is known for its iconic Eiffel Tower.',\n",
    "    'Elon Musk is the CEO of SpaceX and Tesla.',\n",
    "    'The Great Barrier Reef is the world\\'s largest coral reef system, located in Australia.',\n",
    "    'Mount Everest, the highest peak in the world, is part of the Himalayan mountain range.',\n",
    "    'The Mona Lisa, painted by Leonardo da Vinci, is displayed in the Louvre Museum in Paris.',\n",
    "    'Barack Obama served as the 44th President of the United States from 2009 to 2017.',\n",
    "    'The Amazon rainforest, often called the \"lungs of the Earth,\" is vital for global oxygen production.',\n",
    "    'Albert Einstein, a renowned physicist, developed the theory of relativity.',\n",
    "    'The Nile River is the longest river in Africa, flowing through multiple countries.',\n",
    "]\n",
    "\n",
    "\n",
    "for e in example_sentences:\n",
    "    words = e.split()    # Note: assumes white-space tokenization is OK\n",
    "    ner_tags = predict_ner(words)\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        print(f'{word:10s} ➔ {tag}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "To get a better intuitive understanding of tagging results, let's implement a visualization using the[`displacy`](https://explosion.ai/demos/displacy-ent) library.\n",
    "\n",
    "The code here mostly maps the IOB tags to character offets and formats the data for displacy. Unless you're interested in modifying this or otherwise working with this library, there's no need to go through this in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paris, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "the capital of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    France, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "is known for its iconic \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Eiffel Tower.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Musk \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "is the CEO of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SpaceX \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Organization</span>\n",
       "</mark>\n",
       "and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Organization</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Barrier Reef \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "is the world's largest coral reef system, located in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Australia.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mount Everest, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "the highest peak in the world, is part of the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Himalayan \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "mountain range.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mona Lisa, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "painted by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Leonardo da Vinci, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "is displayed in the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Louvre Museum \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paris.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "served as the 44th President of the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "from 2009 to 2017.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon rainforest, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "often called the &quot;lungs of the Earth,&quot; is vital for global oxygen production.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Albert Einstein, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "a renowned physicist, developed the theory of relativity.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nile River \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "is the longest river in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Africa, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       "flowing through multiple countries.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "# Mapping of MultiNERD types for displacy\n",
    "\n",
    "type_map = {\n",
    "    'PER': 'Person',\n",
    "    'LOC': 'Location',\n",
    "    'ORG': 'Organization',\n",
    "    'ANIM': 'Animal',\n",
    "    'BIO': 'Biological entity',\n",
    "    'CEL': 'Celestial Body',\n",
    "    'DIS': 'Disease',\n",
    "    'EVE': 'Event',\n",
    "    'FOOD': 'Food',\n",
    "    'INST': 'Instrument',\n",
    "    'MEDIA': 'Media',\n",
    "    'PLANT': 'Plant',\n",
    "    'MYTH': 'Mythological entity',\n",
    "    'TIME': 'Time',\n",
    "    'VEHI': 'Vehicle',\n",
    "}\n",
    "\n",
    "def render_with_displacy(words, tags):\n",
    "    tagged, offset, start, label = [], 0, None, None\n",
    "    for word, tag in zip(words, tags):\n",
    "        if tag[0] in 'OB' and start is not None:    # current ends\n",
    "            tagged.append({\n",
    "                'start': start,\n",
    "                'end': offset,\n",
    "                'label': type_map.get(label, label)\n",
    "            })\n",
    "            start, label = None, None\n",
    "        if tag[0] == 'B':\n",
    "            start, label = offset, tag[2:]\n",
    "        elif tag[0] == 'I':\n",
    "            if start is None:    # I without B, but nevermind\n",
    "                start, label = offset, tag[2:]\n",
    "        else:\n",
    "            assert tag == 'O', 'unexpected tag {}'.format(tag)\n",
    "        offset += len(word) + 1    # +1 for space\n",
    "    if start:    # span open at sentence end\n",
    "        tagged.append({\n",
    "                'start': start,\n",
    "                'end': offset,\n",
    "                'label': type_map.get(label, label)\n",
    "        })\n",
    "    doc = {\n",
    "        'text': ' '.join(words),\n",
    "        'ents': tagged\n",
    "    }\n",
    "    displacy.render(doc, style='ent', jupyter=True, manual=True)\n",
    "\n",
    "\n",
    "for e in example_sentences:\n",
    "    words = e.split()    # Note: assumes white-space tokenization is OK\n",
    "    ner_tags = predict_ner(words)\n",
    "    render_with_displacy(words, ner_tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Method to Apply to Validation Dataset (& Then Apply it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to('cpu')\n",
    "    attention_mask = batch[\"attention_mask\"].to('cpu')\n",
    "    labels = batch[\"labels\"].to('cpu')\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = trainer.model(input_ids, \n",
    "                               attention_mask\n",
    "                               )\n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        predicted_label = torch.argmax(output.logits, \n",
    "                                       axis=-1\n",
    "                                       ).cpu().numpy()\n",
    "        \n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, 31), \n",
    "                         labels.view(-1), \n",
    "                         reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Above Function to Entire Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 32820/32820 [24:15<00:00, 22.55 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "eval_set = encoded_ds['test']\n",
    "\n",
    "eval_set = eval_set.map(forward_pass_with_label,\n",
    "                        batched=True,\n",
    "                        batch_size=32)\n",
    "\n",
    "eval_df = eval_set.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Up Padding Tokens\n",
    "\n",
    "Defined a placeholder label ID for special tokens (e.g. `[IGN]`) and tokens that represent continuation wordpieces. For example, if the word `Partition` is tokenized into the parts `Part` and `##ition`, the subword token `##ition` would get this ID.\n",
    "\n",
    "(Here the \"magic\" value -100 is significant: this matches the default pythorch `ignore_index`, a value that is ignored in loss functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6464, 18, 239, 20, 339, 621, 1273, 21, 18, 19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[O, O, O, O, B-EVE, I-EVE, I-EVE, O, O, O, O, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-O...</td>\n",
       "      <td>[▁Between, ▁the, ▁end, ▁of, ▁World, ▁War, ▁II,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [6464, 18, 239, 20, 339, 621, 1273, 21, 18, 19...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [O, O, O, O, B-EVE, I-EVE, I-EVE, O, O, O, O, ...   \n",
       "\n",
       "                                                loss  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                     predicted_label  \\\n",
       "0  [B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-ORG, B-O...   \n",
       "\n",
       "                                        input_tokens  \n",
       "0  [▁Between, ▁the, ▁end, ▁of, ▁World, ▁War, ▁II,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[-100] = \"IGN\"\n",
    "eval_df[\"input_tokens\"] = eval_df[\"input_ids\"].apply(\n",
    "    lambda x: tokenizer.convert_ids_to_tokens(x))\n",
    "eval_df[\"predicted_label\"] = eval_df[\"predicted_label\"].apply(\n",
    "    lambda x: [id2label[i] for i in x])\n",
    "eval_df[\"labels\"] = eval_df[\"labels\"].apply(\n",
    "    lambda x: [id2label[i] for i in x])\n",
    "eval_df['loss'] = eval_df.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "eval_df['predicted_label'] = eval_df.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "eval_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unwrap Each Token Within Sample Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6464</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁Between</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B-EVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I-EVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1273</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I-EVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids token_type_ids attention_mask labels  loss predicted_label  \\\n",
       "0      6464              0              1      O   0.0           B-ORG   \n",
       "0        18              0              1      O   0.0           B-ORG   \n",
       "0       239              0              1      O   0.0           B-ORG   \n",
       "0        20              0              1      O   0.0           B-ORG   \n",
       "0       339              0              1  B-EVE   0.0           B-ORG   \n",
       "0       621              0              1  I-EVE   0.0           B-ORG   \n",
       "0      1273              0              1  I-EVE   0.0           B-ORG   \n",
       "\n",
       "  input_tokens  \n",
       "0     ▁Between  \n",
       "0         ▁the  \n",
       "0         ▁end  \n",
       "0          ▁of  \n",
       "0       ▁World  \n",
       "0         ▁War  \n",
       "0          ▁II  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df_tokens = eval_df.apply(pd.Series.explode)\n",
    "eval_df_tokens = eval_df_tokens.query(\"labels != 'IGN'\")\n",
    "eval_df_tokens[\"loss\"] = eval_df_tokens[\"loss\"].astype(float).round(2)\n",
    "eval_df_tokens.head(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Which Tokens Have Accumulated Most Loss in Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_tokens</th>\n",
       "      <td>▁</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁a</td>\n",
       "      <td>▁of</td>\n",
       "      <td>▁to</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁with</td>\n",
       "      <td>▁or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>121976</td>\n",
       "      <td>20850</td>\n",
       "      <td>35218</td>\n",
       "      <td>13900</td>\n",
       "      <td>22056</td>\n",
       "      <td>12762</td>\n",
       "      <td>38</td>\n",
       "      <td>16374</td>\n",
       "      <td>6058</td>\n",
       "      <td>3416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>3.528</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>4011.37</td>\n",
       "      <td>275.17</td>\n",
       "      <td>257.03</td>\n",
       "      <td>186.62</td>\n",
       "      <td>170.33</td>\n",
       "      <td>142.56</td>\n",
       "      <td>134.08</td>\n",
       "      <td>121.77</td>\n",
       "      <td>119.1</td>\n",
       "      <td>118.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1       2       3       4       5       6       7  \\\n",
       "input_tokens        ▁    ▁and    ▁the      ▁a     ▁of     ▁to     ▁Qu     ▁in   \n",
       "count          121976   20850   35218   13900   22056   12762      38   16374   \n",
       "mean            0.033   0.013   0.007   0.013   0.008   0.011   3.528   0.007   \n",
       "sum           4011.37  275.17  257.03  186.62  170.33  142.56  134.08  121.77   \n",
       "\n",
       "                  8       9  \n",
       "input_tokens  ▁with     ▁or  \n",
       "count          6058    3416  \n",
       "mean           0.02   0.035  \n",
       "sum           119.1  118.66  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    eval_df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Which Label IDs Have Most Loss in Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>B-BIO</td>\n",
       "      <td>I-TIME</td>\n",
       "      <td>B-VEHI</td>\n",
       "      <td>I-VEHI</td>\n",
       "      <td>I-DIS</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-FOOD</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>...</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-MEDIA</td>\n",
       "      <td>I-CEL</td>\n",
       "      <td>I-BIO</td>\n",
       "      <td>I-INST</td>\n",
       "      <td>I-MYTH</td>\n",
       "      <td>B-EVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>830</td>\n",
       "      <td>1154</td>\n",
       "      <td>28</td>\n",
       "      <td>508</td>\n",
       "      <td>160</td>\n",
       "      <td>174</td>\n",
       "      <td>2502</td>\n",
       "      <td>2268</td>\n",
       "      <td>1656</td>\n",
       "      <td>2360</td>\n",
       "      <td>...</td>\n",
       "      <td>7824</td>\n",
       "      <td>15984</td>\n",
       "      <td>15700</td>\n",
       "      <td>15014</td>\n",
       "      <td>2818</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>18</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>506.44</td>\n",
       "      <td>479.84</td>\n",
       "      <td>8.18</td>\n",
       "      <td>75.45</td>\n",
       "      <td>21.98</td>\n",
       "      <td>20.44</td>\n",
       "      <td>283.74</td>\n",
       "      <td>220.75</td>\n",
       "      <td>153.39</td>\n",
       "      <td>185.36</td>\n",
       "      <td>...</td>\n",
       "      <td>11.41</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.38</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1      2       3       4       5       6       7   \\\n",
       "labels  I-PLANT  I-ANIM  B-BIO  I-TIME  B-VEHI  I-VEHI   I-DIS  B-ANIM   \n",
       "count       830    1154     28     508     160     174    2502    2268   \n",
       "mean       0.61   0.416  0.292   0.149   0.137   0.117   0.113   0.097   \n",
       "sum      506.44  479.84   8.18   75.45   21.98   20.44  283.74  220.75   \n",
       "\n",
       "            8        9   ...     21     22     23     24       25     26  \\\n",
       "labels  I-FOOD  B-PLANT  ...  I-ORG  I-PER  B-LOC  B-PER  I-MEDIA  I-CEL   \n",
       "count     1656     2360  ...   7824  15984  15700  15014     2818     38   \n",
       "mean     0.093    0.079  ...  0.001    0.0    0.0    0.0      0.0    0.0   \n",
       "sum     153.39   185.36  ...  11.41   5.82   5.38   1.24     0.02    0.0   \n",
       "\n",
       "           27      28      29     30  \n",
       "labels  I-BIO  I-INST  I-MYTH  B-EVE  \n",
       "count       8      44      18    598  \n",
       "mean      0.0     0.0     0.0    0.0  \n",
       "sum       0.0     0.0     0.0    0.0  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    eval_df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    "    .fillna(0)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Function to Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "\n",
    "    # Set tick label font size\n",
    "    plt.tick_params(axis='both', which='both', labelsize=10)\n",
    "\n",
    "    # Value name for rotating horizontally\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Set title\n",
    "    plt.title(\"Normalized Confusion Matrix of XLNET-NER-SystemA\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_df_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m eval_token_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(eval_df_tokens[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m eval_df_tokens \u001b[39m=\u001b[39m eval_df_tokens[eval_df_tokens[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mB-PER\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemA.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m eval_df_tokens \u001b[39m=\u001b[39m eval_df_tokens[eval_df_tokens[\u001b[39m'\u001b[39m\u001b[39mpredicted_label\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mB-PER\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_df_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "eval_token_list = list(set(eval_df_tokens['labels']))\n",
    "eval_df_tokens = eval_df_tokens[eval_df_tokens['labels'] != 'B-PER']\n",
    "eval_df_tokens = eval_df_tokens[eval_df_tokens['predicted_label'] != 'B-PER']\n",
    "plot_confusion_matrix(eval_df_tokens[\"labels\"], eval_df_tokens[\"predicted_label\"],\n",
    "                      eval_token_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define & Call Function to Display Example Token Sequences Along With Labels & Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁species</td>\n",
       "      <td>▁include</td>\n",
       "      <td>▁Coast</td>\n",
       "      <td>▁Live</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁a</td>\n",
       "      <td>gri</td>\n",
       "      <td>folia</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁Engel</td>\n",
       "      <td>mann</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁</td>\n",
       "      <td>eng</td>\n",
       "      <td>elman</td>\n",
       "      <td>ni</td>\n",
       "      <td>i</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁Canyon</td>\n",
       "      <td>▁Live</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁</td>\n",
       "      <td>ch</td>\n",
       "      <td>ry</td>\n",
       "      <td>s</td>\n",
       "      <td>ole</td>\n",
       "      <td>pi</td>\n",
       "      <td>s</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁Baja</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁peninsula</td>\n",
       "      <td>ris</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;sep&gt;</td>\n",
       "      <td>&lt;cls&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.72</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.78</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1        2        3        4        5     6        7   \\\n",
       "tokens  ▁species  ▁include   ▁Coast    ▁Live     ▁Oak        ▁     (        ▁   \n",
       "labels         O         O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds          O         O        O        O        O        O     O  B-PLANT   \n",
       "losses      0.00      0.00     0.00     0.00     0.04     0.00  0.00     0.27   \n",
       "\n",
       "             8        9     10    11       12    13       14       15  \\\n",
       "tokens        \"      ▁Qu    er   cus       ▁a   gri    folia        ▁   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN  I-PLANT   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.39     0.20  8.40  0.00     8.17  0.00     7.24     0.00   \n",
       "\n",
       "             16       17       18       19    20      21    22      23  \\\n",
       "tokens        \"        ▁        )        ▁     ,  ▁Engel  mann    ▁Oak   \n",
       "labels      IGN  I-PLANT      IGN        O   IGN  B-ANIM   IGN  I-ANIM   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT     O       O     O       O   \n",
       "losses     0.00     0.06     0.00     0.00  7.99    0.00  8.56    0.00   \n",
       "\n",
       "            24    25       26       27       28    29    30      31    32  \\\n",
       "tokens       ▁     (        ▁        \"      ▁Qu    er   cus       ▁   eng   \n",
       "labels  I-ANIM   IGN   I-ANIM      IGN   I-ANIM   IGN   IGN  I-ANIM   IGN   \n",
       "preds        O     O  B-PLANT  I-PLANT  I-PLANT     O     O       O     O   \n",
       "losses    0.00  0.00     4.74     0.00     4.72  9.20  0.00    8.91  0.00   \n",
       "\n",
       "             33       34       35       36       37       38       39  \\\n",
       "tokens    elman       ni        i        ▁        \"        ▁        )   \n",
       "labels      IGN      IGN      IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses     9.26     0.00     0.00     3.73     0.00     0.00     0.00   \n",
       "\n",
       "             40    41       42       43       44       45    46       47  \\\n",
       "tokens        ▁     ,  ▁Canyon    ▁Live     ▁Oak        ▁     (        ▁   \n",
       "labels        O   IGN  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds   I-PLANT     O        O        O        O        O     O  B-PLANT   \n",
       "losses     0.00  9.03     0.00     9.64     0.00     0.00  0.00     0.15   \n",
       "\n",
       "             48       49    50    51       52    53       54       55  \\\n",
       "tokens        \"      ▁Qu    er   cus        ▁    ch       ry        s   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN      IGN   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.27     0.17  8.33  0.00     8.14  0.00     7.41     0.00   \n",
       "\n",
       "             56       57       58       59       60       61       62    63  \\\n",
       "tokens      ole       pi        s        ▁        \"        ▁        )     ▁   \n",
       "labels      IGN      IGN      IGN  I-PLANT      IGN  I-PLANT      IGN     O   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT     O   \n",
       "losses     0.00     0.10     0.00     0.00     0.00     0.00     0.00  0.00   \n",
       "\n",
       "          64    65      66      67      68    69      70       71       72  \\\n",
       "tokens     ,  ▁and   ▁Baja    ▁Oak       ▁     (       ▁        \"      ▁Qu   \n",
       "labels   IGN     O  B-ANIM  I-ANIM  I-ANIM   IGN  I-ANIM      IGN   I-ANIM   \n",
       "preds      O     O       O       O       O     O       O  B-PLANT  I-PLANT   \n",
       "losses  7.54  0.00    8.14    0.00    0.00  0.00    0.00     4.41     4.78   \n",
       "\n",
       "          73    74          75    76       77       78       79       80  \\\n",
       "tokens    er   cus  ▁peninsula   ris        ▁        \"        ▁        )   \n",
       "labels   IGN   IGN      I-ANIM   IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds      O     O           O     O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses  9.20  0.00        8.67  0.00     9.59     0.00     0.00     4.23   \n",
       "\n",
       "             81    82     83     84  \n",
       "tokens        ▁     .  <sep>  <cls>  \n",
       "labels        O   IGN    IGN    IGN  \n",
       "preds   I-PLANT     O      O      O  \n",
       "losses     0.00  8.83   0.00   9.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁species</td>\n",
       "      <td>▁include</td>\n",
       "      <td>▁Coast</td>\n",
       "      <td>▁Live</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁a</td>\n",
       "      <td>gri</td>\n",
       "      <td>folia</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁Engel</td>\n",
       "      <td>mann</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁</td>\n",
       "      <td>eng</td>\n",
       "      <td>elman</td>\n",
       "      <td>ni</td>\n",
       "      <td>i</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁Canyon</td>\n",
       "      <td>▁Live</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁</td>\n",
       "      <td>ch</td>\n",
       "      <td>ry</td>\n",
       "      <td>s</td>\n",
       "      <td>ole</td>\n",
       "      <td>pi</td>\n",
       "      <td>s</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁Baja</td>\n",
       "      <td>▁Oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁peninsula</td>\n",
       "      <td>ris</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;sep&gt;</td>\n",
       "      <td>&lt;cls&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.72</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.78</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1        2        3        4        5     6        7   \\\n",
       "tokens  ▁species  ▁include   ▁Coast    ▁Live     ▁Oak        ▁     (        ▁   \n",
       "labels         O         O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds          O         O        O        O        O        O     O  B-PLANT   \n",
       "losses      0.00      0.00     0.00     0.00     0.04     0.00  0.00     0.27   \n",
       "\n",
       "             8        9     10    11       12    13       14       15  \\\n",
       "tokens        \"      ▁Qu    er   cus       ▁a   gri    folia        ▁   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN  I-PLANT   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.39     0.20  8.40  0.00     8.17  0.00     7.24     0.00   \n",
       "\n",
       "             16       17       18       19    20      21    22      23  \\\n",
       "tokens        \"        ▁        )        ▁     ,  ▁Engel  mann    ▁Oak   \n",
       "labels      IGN  I-PLANT      IGN        O   IGN  B-ANIM   IGN  I-ANIM   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT     O       O     O       O   \n",
       "losses     0.00     0.06     0.00     0.00  7.99    0.00  8.56    0.00   \n",
       "\n",
       "            24    25       26       27       28    29    30      31    32  \\\n",
       "tokens       ▁     (        ▁        \"      ▁Qu    er   cus       ▁   eng   \n",
       "labels  I-ANIM   IGN   I-ANIM      IGN   I-ANIM   IGN   IGN  I-ANIM   IGN   \n",
       "preds        O     O  B-PLANT  I-PLANT  I-PLANT     O     O       O     O   \n",
       "losses    0.00  0.00     4.74     0.00     4.72  9.20  0.00    8.91  0.00   \n",
       "\n",
       "             33       34       35       36       37       38       39  \\\n",
       "tokens    elman       ni        i        ▁        \"        ▁        )   \n",
       "labels      IGN      IGN      IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses     9.26     0.00     0.00     3.73     0.00     0.00     0.00   \n",
       "\n",
       "             40    41       42       43       44       45    46       47  \\\n",
       "tokens        ▁     ,  ▁Canyon    ▁Live     ▁Oak        ▁     (        ▁   \n",
       "labels        O   IGN  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   \n",
       "preds   I-PLANT     O        O        O        O        O     O  B-PLANT   \n",
       "losses     0.00  9.03     0.00     9.64     0.00     0.00  0.00     0.15   \n",
       "\n",
       "             48       49    50    51       52    53       54       55  \\\n",
       "tokens        \"      ▁Qu    er   cus        ▁    ch       ry        s   \n",
       "labels      IGN  I-PLANT   IGN   IGN  I-PLANT   IGN      IGN      IGN   \n",
       "preds   I-PLANT  I-PLANT     O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     0.27     0.17  8.33  0.00     8.14  0.00     7.41     0.00   \n",
       "\n",
       "             56       57       58       59       60       61       62    63  \\\n",
       "tokens      ole       pi        s        ▁        \"        ▁        )     ▁   \n",
       "labels      IGN      IGN      IGN  I-PLANT      IGN  I-PLANT      IGN     O   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT     O   \n",
       "losses     0.00     0.10     0.00     0.00     0.00     0.00     0.00  0.00   \n",
       "\n",
       "          64    65      66      67      68    69      70       71       72  \\\n",
       "tokens     ,  ▁and   ▁Baja    ▁Oak       ▁     (       ▁        \"      ▁Qu   \n",
       "labels   IGN     O  B-ANIM  I-ANIM  I-ANIM   IGN  I-ANIM      IGN   I-ANIM   \n",
       "preds      O     O       O       O       O     O       O  B-PLANT  I-PLANT   \n",
       "losses  7.54  0.00    8.14    0.00    0.00  0.00    0.00     4.41     4.78   \n",
       "\n",
       "          73    74          75    76       77       78       79       80  \\\n",
       "tokens    er   cus  ▁peninsula   ris        ▁        \"        ▁        )   \n",
       "labels   IGN   IGN      I-ANIM   IGN   I-ANIM      IGN   I-ANIM      IGN   \n",
       "preds      O     O           O     O  B-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses  9.20  0.00        8.67  0.00     9.59     0.00     0.00     4.23   \n",
       "\n",
       "             81    82     83     84  \n",
       "tokens        ▁     .  <sep>  <cls>  \n",
       "labels        O   IGN    IGN    IGN  \n",
       "preds   I-PLANT     O      O      O  \n",
       "losses     0.00  8.83   0.00   9.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁of</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁chap</td>\n",
       "      <td>ar</td>\n",
       "      <td>ral</td>\n",
       "      <td>▁woodland</td>\n",
       "      <td>▁species</td>\n",
       "      <td>▁include</td>\n",
       "      <td>▁</td>\n",
       "      <td>:</td>\n",
       "      <td>▁canyon</td>\n",
       "      <td>▁live</td>\n",
       "      <td>▁oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁</td>\n",
       "      <td>ch</td>\n",
       "      <td>ry</td>\n",
       "      <td>s</td>\n",
       "      <td>ole</td>\n",
       "      <td>pi</td>\n",
       "      <td>s</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁valley</td>\n",
       "      <td>▁oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁</td>\n",
       "      <td>lob</td>\n",
       "      <td>ata</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁blue</td>\n",
       "      <td>▁oak</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Qu</td>\n",
       "      <td>er</td>\n",
       "      <td>cus</td>\n",
       "      <td>▁do</td>\n",
       "      <td>ug</td>\n",
       "      <td>las</td>\n",
       "      <td>ii</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁gray</td>\n",
       "      <td>▁pine</td>\n",
       "      <td>▁</td>\n",
       "      <td>(</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁Pin</td>\n",
       "      <td>us</td>\n",
       "      <td>▁</td>\n",
       "      <td>sa</td>\n",
       "      <td>bin</td>\n",
       "      <td>iana</td>\n",
       "      <td>▁</td>\n",
       "      <td>\"</td>\n",
       "      <td>▁</td>\n",
       "      <td>)</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;sep&gt;</td>\n",
       "      <td>&lt;cls&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ANIM</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>I-PLANT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.80</td>\n",
       "      <td>11.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.52</td>\n",
       "      <td>4.32</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.01</td>\n",
       "      <td>5.80</td>\n",
       "      <td>10.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2     3     4          5         6         7     8   \\\n",
       "tokens   ▁of  ▁the  ▁chap    ar   ral  ▁woodland  ▁species  ▁include     ▁   \n",
       "labels     O     O      O   IGN   IGN          O         O         O     O   \n",
       "preds      O     O      O     O     O          O         O         O     O   \n",
       "losses  0.00  0.00   0.00  0.00  0.00       0.00      0.00      0.00  0.00   \n",
       "\n",
       "          9        10       11       12       13    14       15    16  \\\n",
       "tokens     :  ▁canyon    ▁live     ▁oak        ▁     (        ▁     \"   \n",
       "labels   IGN  B-PLANT  I-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   IGN   \n",
       "preds      O        O        O        O        O     O        O     O   \n",
       "losses  0.00     3.04     4.02     3.80    11.31  0.00    11.28  0.00   \n",
       "\n",
       "             17       18       19       20       21       22       23  \\\n",
       "tokens      ▁Qu       er      cus        ▁       ch       ry        s   \n",
       "labels  I-PLANT      IGN      IGN  I-PLANT      IGN      IGN      IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT   \n",
       "losses     9.23     0.00     0.00     0.04     0.00     0.00     0.00   \n",
       "\n",
       "             24       25    26       27    28       29    30    31    32  \\\n",
       "tokens      ole       pi     s        ▁     \"        ▁     )     ▁     ,   \n",
       "labels      IGN      IGN   IGN  I-PLANT   IGN  I-PLANT   IGN     O   IGN   \n",
       "preds   I-PLANT  I-PLANT     O        O     O        O     O     O     O   \n",
       "losses     0.00     0.00  0.00     9.79  0.00    11.01  0.00  0.00  0.00   \n",
       "\n",
       "             33       34       35    36       37    38       39       40  \\\n",
       "tokens  ▁valley     ▁oak        ▁     (        ▁     \"      ▁Qu       er   \n",
       "labels  B-PLANT  I-PLANT  I-PLANT   IGN  I-PLANT   IGN  I-PLANT      IGN   \n",
       "preds         O        O        O     O        O     O  B-PLANT  I-PLANT   \n",
       "losses     3.52     4.32    10.81  0.00    11.45  0.00     9.31     0.00   \n",
       "\n",
       "             41       42       43       44       45    46       47    48  \\\n",
       "tokens      cus        ▁      lob      ata        ▁     \"        ▁     )   \n",
       "labels      IGN  I-PLANT      IGN      IGN  I-PLANT   IGN  I-PLANT   IGN   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT        O     O        O     O   \n",
       "losses     0.00     0.05     0.00     0.00     9.94  0.00    11.00  0.00   \n",
       "\n",
       "          49    50      51      52      53    54      55    56       57  \\\n",
       "tokens     ▁     ,   ▁blue    ▁oak       ▁     (       ▁     \"      ▁Qu   \n",
       "labels     O   IGN  B-ANIM  I-ANIM  I-ANIM   IGN  I-ANIM   IGN   I-ANIM   \n",
       "preds      O     O       O       O       O     O       O     O  B-PLANT   \n",
       "losses  0.00  0.00    4.01    5.80   10.35  0.00   10.90  0.00    10.83   \n",
       "\n",
       "             58       59       60       61       62       63      64    65  \\\n",
       "tokens       er      cus      ▁do       ug      las       ii       ▁     \"   \n",
       "labels      IGN      IGN   I-ANIM      IGN      IGN      IGN  I-ANIM   IGN   \n",
       "preds   I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT       O     O   \n",
       "losses     0.00     0.00     3.68     0.00     0.00     0.00    9.84  0.00   \n",
       "\n",
       "            66    67    68    69    70     71     72    73    74    75    76  \\\n",
       "tokens       ▁     )     ▁     ,  ▁and  ▁gray  ▁pine     ▁     (     ▁     \"   \n",
       "labels  I-ANIM   IGN     O   IGN     O      O      O     O   IGN     O   IGN   \n",
       "preds        O     O     O     O     O      O      O     O     O     O     O   \n",
       "losses   10.85  0.00  0.00  0.00  0.00   0.15   0.12  0.00  0.00  0.00  0.00   \n",
       "\n",
       "             77       78       79       80       81       82    83    84  \\\n",
       "tokens     ▁Pin       us        ▁       sa      bin     iana     ▁     \"   \n",
       "labels        O      IGN        O      IGN      IGN      IGN     O   IGN   \n",
       "preds   B-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT  I-PLANT     O     O   \n",
       "losses     4.81     0.00     4.33     0.00     0.00     0.00  0.00  0.00   \n",
       "\n",
       "          85    86    87    88     89     90  \n",
       "tokens     ▁     )     ▁     .  <sep>  <cls>  \n",
       "labels     O   IGN     O   IGN    IGN    IGN  \n",
       "preds      O     O     O     O      O      O  \n",
       "losses  0.00  0.00  0.00  0.00   0.00   0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        eval_df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n",
    "                               \"preds\": preds, \"losses\": losses}).T\n",
    "        yield eval_df_tmp\n",
    "\n",
    "eval_df[\"total_loss\"] = eval_df[\"loss\"].apply(sum)\n",
    "eval_df_tmp = eval_df.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "for sample in get_samples(eval_df_tmp):\n",
    "    display(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
