{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition Task SystemB (MultiNERD Dataset)\n",
    "\n",
    "### XLNET Base Cased\n",
    "\n",
    "[MultiNERD Dataset] ðŸ¤—: https://huggingface.co/datasets/Babelscape/multinerd \n",
    "\n",
    "[XLNET Base Cased Model] ðŸ¤—: https://huggingface.co/xlnet-base-cased"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries\n",
    "\n",
    "* [AutoTokenizer](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer): A tokenizer class designed to accommodate the tokenization conventions of various pre-trained models.\n",
    "\n",
    "* [AutoModelForTokenClassification](https://huggingface.co/transformers/model_doc/auto.html#automodelfortokenclassification): An extension of the [AutoModel](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel) class, capable of loading diverse pre-trained models. It supports fine-tuning for the classification of each token within a sequence.\n",
    "\n",
    "* [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments): A straightforward class tailored for storing hyperparameters and other settings essential for model training.\n",
    "\n",
    "* [Trainer](https://huggingface.co/transformers/main_classes/trainer.html): A versatile class that facilitates various forms of training for transformer models.\n",
    "\n",
    "* [DataCollatorForTokenClassification](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py): A class designed for padding token classification examples to the same length during training.\n",
    "\n",
    "* [load_dataset](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset): A function crafted for effortlessly loading datasets, including those from the [datasets](https://huggingface.co/datasets) collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Dataset into only five entity types and the O tagas as input of SystemB\n",
    "\n",
    "* First load the dataset with [`load_dataset`](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset), then all examples should thus remain, but entity types not belonging to one of the following five should be set to zero: PERSON(PER), ORGANIZATION(ORG), LOCATION(LOC), DISEASES(DIS), \n",
    "ANIMAL(ANIM)\n",
    "\n",
    "* System B supposed to be a fine-tuned XLNET model on the all examples with five entity types.\n",
    "\n",
    "* The loaded dataset is a dictionary-like container for [`Dataset`](https://huggingface.co/docs/datasets/exploring.html) objects for training, development (validation), and test data. We'll here be using the `tokens` and `ner_tags` fields and ignoring `lang` (language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 14.40it/s]\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 54506.87it/s]\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 149796.57it/s]\n"
     ]
    }
   ],
   "source": [
    "original_dataset = load_dataset('Babelscape/multinerd')\n",
    "label_map = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-ANIM\": 7,\n",
    "    \"I-ANIM\": 8,\n",
    "    \"B-DIS\": 13,\n",
    "    \"I-DIS\": 14\n",
    "}\n",
    "label_list = [\"O\",\"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-ANIM\", \"I-ANIM\", \"B-DIS\", \"I-DIS\"]\n",
    "\n",
    "valid_entity_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "def filter_and_transform(example):\n",
    "    filtered_ner_tags = []\n",
    "    for tag in example['ner_tags']:\n",
    "        if tag in list(valid_entity_map.keys()) and tag ==13:\n",
    "            filtered_ner_tags.append(9)  \n",
    "        elif tag in list(valid_entity_map.keys()) and tag ==14:\n",
    "            filtered_ner_tags.append(10)  \n",
    "        elif tag in list(valid_entity_map.keys()):\n",
    "            filtered_ner_tags.append(tag) \n",
    "        else:\n",
    "            filtered_ner_tags.append(0) \n",
    "    \n",
    "    return {\n",
    "        'tokens': example['tokens'],\n",
    "        'ner_tags': filtered_ner_tags,\n",
    "        'lang': example['lang']\n",
    "    }\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    original_dataset[split] = original_dataset[split].map(filter_and_transform)\n",
    "\n",
    "# Generate a list of numbers\n",
    "num_labels = len(label_list)\n",
    "\n",
    "data = original_dataset.remove_columns([\"lang\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2678400/2678400 [01:15<00:00, 35263.51 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334800/334800 [00:09<00:00, 35063.10 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335986/335986 [00:09<00:00, 36358.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (133920, 2)\n",
      "Testing data shape: (0, 2)\n",
      "Validation data shape: (16740, 2)\n"
     ]
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    'train': data['train'].filter(lambda example, idx: idx % 20 == 0, with_indices=True), \n",
    "    'eval': data['validation'].filter(lambda example, idx: idx % 20 == 0, with_indices=True), \n",
    "    'test': data['test'].filter(lambda example, idx: idx % 10 == 20, with_indices=True)})\n",
    "\n",
    "\n",
    "print('Training data shape:', ds['train'].shape)\n",
    "print('Testing data shape:', ds['test'].shape)\n",
    "print('Validation data shape:', ds['eval'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['John',\n",
       "  'Locke',\n",
       "  'verwendet',\n",
       "  'diese',\n",
       "  'Vorstellung',\n",
       "  'als',\n",
       "  'Metapher',\n",
       "  'fÃ¼r',\n",
       "  'den',\n",
       "  'menschlichen',\n",
       "  'Verstand',\n",
       "  'bei',\n",
       "  'der',\n",
       "  'Geburt',\n",
       "  'eines',\n",
       "  'Kindes',\n",
       "  '.'],\n",
       " 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds['train'][12]\n",
    "\n",
    "example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Feature Information About Each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: \n",
      "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "\n",
      "ner_tags: \n",
      "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in ds[\"train\"].features.items():\n",
    "    print(f\"{k}: \\n{v}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Tag Values & Conversions Between String & Integer Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tag values: \n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-ANIM', 'I-ANIM', 'B-DIS', 'I-DIS']\n",
      "Number of NER Tags: \n",
      "11\n",
      "id2label: \n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-ANIM', 8: 'I-ANIM', 9: 'B-DIS', 10: 'I-DIS'}\n",
      "label2id: \n",
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-ANIM': 7, 'I-ANIM': 8, 'B-DIS': 9, 'I-DIS': 10}\n"
     ]
    }
   ],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-ANIM\": 7,\n",
    "    \"I-ANIM\": 8,\n",
    "    \"B-DIS\": 9,\n",
    "    \"I-DIS\": 10\n",
    "}\n",
    "\n",
    "id2label = {tag: idx for idx, tag in label2id.items()}\n",
    "\n",
    "pos_tag_values = list(label2id.keys())\n",
    "NUM_OF_LABELS = len(pos_tag_values)\n",
    "\n",
    "print(f\"List of tag values: \\n{pos_tag_values}\")\n",
    "print(f\"Number of NER Tags: \\n{NUM_OF_LABELS}\")\n",
    "print(f\"id2label: \\n{id2label}\")\n",
    "print(f\"label2id: \\n{label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text classification tasks discussed earlier, each data example included a string representing an entire sentence. In contrast, the current data is already segmented into words, denoted by the key `tokens` in the `Dataset` object. Interestingly, the tags in the data correspond to these individual words. Let's examine a sample sentence for better clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002     âž” O\n",
      "ging     âž” O\n",
      "er       âž” O\n",
      "ins      âž” O\n",
      "Ausland  âž” O\n",
      "und      âž” O\n",
      "wechselte âž” O\n",
      "fÃ¼r      âž” O\n",
      "750.000  âž” O\n",
      "Pfund    âž” O\n",
      "Sterling âž” O\n",
      "zu       âž” O\n",
      "Manchester âž” B-ORG\n",
      "City     âž” I-ORG\n",
      ".        âž” O\n"
     ]
    }
   ],
   "source": [
    "train_words = ds['train']['tokens']\n",
    "train_tags = ds['train']['ner_tags']\n",
    "for word, tag in zip(train_words[0], train_tags[0]):\n",
    "    print(f'{word:8s} âž” {label_list[tag]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aligning labels (tags) with words, especially when they don't align perfectly with the tokenization of the model, introduces significant complexity. This becomes more apparent once we've loaded a tokenizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental Constants and Parameters Configuration\n",
    "\n",
    "Let's establish critical global variables encompassing the designation of the pre-trained model and the essential hyperparameters instrumental for its meticulous fine-tuning:\n",
    "\n",
    "* `MODEL_CKPT`: The nomenclature of a pre-trained model accessible within the [model repository](https://huggingface.co/models).\n",
    "* `REPORTS_TO`: Incorporating visualization tools vital for comprehensive machine learning experimentation, opt for [TensorBoard](https://www.tensorflow.org/tensorboard) in this context.\n",
    "* `LR`, `WEIGHT_DECAY`, `BATCH_SIZE`, `STEPS`, and `NUM_OF_EPOCHS`: Configurable hyperparameters governing the nuanced fine-tuning process of the model. (Experimenting with diverse values is encouraged!)\n",
    "* `OUTPUT_DIR` and `LOG_DIR`: The designated directories for storing the model and its associated parameters.\n",
    "* `DEVICE` : The computational device on which the model training is executed. In this instance, utilize GPU for optimal training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "MODEL_CKPT = \"xlnet-base-cased\"\n",
    "MODEL_NAME = f\"{MODEL_CKPT}-finetuned-MultiNERD-SystemB\"\n",
    "\n",
    "NUM_OF_EPOCHS = 2\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "STRATEGY = \"epoch\"\n",
    "REPORTS_TO = \"tensorboard\"\n",
    "\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR = 2e-5\n",
    "\n",
    "STEPS = 1250\n",
    "\n",
    "OUTPUT_DIR = f'/srv/users/rudxia/Developer_NLP/notebooks/results/Outdir/{MODEL_CKPT}-finetuned-MultiNERD-SystemB'\n",
    "LOG_DIR= f'/srv/users/rudxia/Developer_NLP/notebooks/results/Log/{MODEL_CKPT}-finetuned-MultiNERD-SystemB'\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Tokenize & Align Inputs\n",
    "\n",
    "Ensuring the alignment of labels (tags) with words that might not align perfectly with the model involves loading a suitable tokenizer using [`AutoTokenizer.from_pretrained`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer.from_pretrained). Specifically, we opt for a [\"fast\" tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html) since it offers a mapping from tokens to input words, a crucial requirement for Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "# def tokenize_and_align_labels(samples):\n",
    "#     tokenized_inputs = tokenizer(samples[\"tokens\"], \n",
    "#                                       truncation=True, \n",
    "#                                       is_split_into_words=True)\n",
    "    \n",
    "#     labels = []\n",
    "    \n",
    "#     for idx, label in enumerate(samples[f\"ner_tags\"]):\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "#         prev_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids: # set special tokens to -100\n",
    "#             if word_idx is None or word_idx == prev_word_idx:\n",
    "#                 label_ids.append(-100)\n",
    "#             else:\n",
    "#                 # Check if word_idx is within the range of the label list\n",
    "#                 if 0 <= word_idx < len(label):\n",
    "#                     label_ids.append(label[word_idx])\n",
    "#                 else:\n",
    "#                     label_ids.append(-100)\n",
    "\n",
    "#             prev_word_idx = word_idx\n",
    "#         labels.append(label_ids)\n",
    "    \n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "def tokenize_and_align_labels(samples):\n",
    "    tokenized_inputs = tokenizer(samples[\"tokens\"], \n",
    "                                      truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for idx, label in enumerate(samples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        prev_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids: # set special tokens to -100\n",
    "            if word_idx is None or word_idx == prev_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            prev_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Above Function to Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133920/133920 [00:29<00:00, 4584.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16740/16740 [00:03<00:00, 4340.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_ds = ds.map(tokenize_and_align_labels, \n",
    "                    batched=True, \n",
    "                    remove_columns=\n",
    "                        [\n",
    "                            'ner_tags', \n",
    "                            'tokens'\n",
    "                        ]\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    num_labels=NUM_OF_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    ).to(DEVICE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = pos_tag_values\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "labels = [label_list[i] for i in example[f'ner_tags']]\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, \n",
    "                            axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, \n",
    "                              references=true_labels)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    OUTPUT_DIR,    # output directory for checkpoints and predictions\n",
    "    MODEL_NAME,\n",
    "    log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    "    logging_dir = LOG_DIR,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=NUM_OF_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    report_to=REPORTS_TO,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    save_strategy=STRATEGY,\n",
    "    hub_private_repo=False,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Trainer and Subclass Trainer to Handle Class Imbalance\n",
    "\n",
    "To fine-tune the pre-trained model, we'll instantiate a [`Trainer`](https://huggingface.co/transformers/main_classes/trainer.html) object. This involves providing the pre-trained model, configuration settings, training and validation datasets, and the previously defined evaluation metric.\n",
    "\n",
    "Most of this should be familiar from the sentence classification notebook, with one notable exception: we include a [`DataCollatorForTokenClassification`](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py) with the trainer. This class efficiently handles the padding of token classification examples in batches, ensuring they are of the same length, as required by PyTorch. The tokenizer's `[PAD]` symbol is employed for padding the output, along with the specified `label_pad_token_id` for padding the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, \n",
    "                     model, \n",
    "                     inputs, \n",
    "                     return_outputs=False):\n",
    "        \n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(\n",
    "            [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \n",
    "             9.0, 10.0, 11.0], \n",
    "            device=DEVICE)\n",
    "        )\n",
    "        loss = loss_fct(logits.view(-1, \n",
    "                                    self.model.config.num_labels), \n",
    "                        labels.view(-1)\n",
    "                        )\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "trainer = CustomTrainer(model, \n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer,\n",
    "                  train_dataset=encoded_ds[\"train\"],\n",
    "                  eval_dataset=encoded_ds[\"eval\"],\n",
    "                  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='11160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   70/11160 00:23 < 1:02:55, 2.94 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 1651, in forward\n    outputs = self.transformer(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 1237, in forward\n    outputs = layer_module(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 508, in forward\n    outputs = self.rel_attn(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 439, in forward\n    attn_vec = self.rel_attn_core(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 291, in rel_attn_core\n    attn_score = (ac + bd + ef) * self.scale\nRuntimeError: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 47.74 GiB total capacity; 40.79 GiB already allocated; 952.31 MiB free; 41.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[1;32m/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb Cell 33\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/MultiNERD_XLNET_SystemB.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# compute custom loss \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:86\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 86\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     87\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 1651, in forward\n    outputs = self.transformer(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 1237, in forward\n    outputs = layer_module(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 508, in forward\n    outputs = self.rel_attn(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 439, in forward\n    attn_vec = self.rel_attn_core(\n  File \"/srv/users/rudxia/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 291, in rel_attn_core\n    attn_score = (ac + bd + ef) * self.scale\nRuntimeError: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 47.74 GiB total capacity; 40.79 GiB already allocated; 952.31 MiB free; 41.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = \"/srv/users/rudxia/Developer_NLP/notebooks/results/saved_model/XLNET_MultiNERD_SystemB\"\n",
    "trainer.save_model(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/DunnBC22/xlnet-base-cased-finetuned-WikiNeural-PoS\n",
      "   ae0d726..6227b91  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         2.0\n",
      "  train_loss               =      0.1087\n",
      "  train_runtime            = 18:01:07.79\n",
      "  train_samples_per_second =       2.859\n",
      "  train_steps_per_second   =       0.179\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model is now available as `trainer.model`. Here we define functions to predict tags for a user-defined string.\n",
    "\n",
    "The main complexity here arises from the need to map back from token labels to word labels, inverting the mapping performed in `encode_dataset`. The process is basically the same as in `compute_metrics` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "model.eval()    # switch to evaluation mode\n",
    "model.to('cpu')    # switch to CPU\n",
    "\n",
    "\n",
    "def word_start_tokens(tokenized):\n",
    "    \"\"\"Return list of bool identifying which tokens start words.\"\"\"\n",
    "    prev_word_idx = None\n",
    "    is_word_start = []\n",
    "    for word_idx in tokenized.word_ids():\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            is_word_start.append(False)\n",
    "        else:\n",
    "            is_word_start.append(True)\n",
    "        prev_word_idx = word_idx\n",
    "    return is_word_start\n",
    "\n",
    "\n",
    "def predict_ner(words):\n",
    "    tokenized = tokenizer(words, is_split_into_words=True, return_tensors='pt')\n",
    "    pred = model(**tokenized)\n",
    "    pred_idx = pred.logits.detach().numpy().argmax(axis=2)\n",
    "    token_labels = [label_list[i] for s in pred_idx for i in s]\n",
    "    word_labels = []\n",
    "    for label, is_word_start in zip(token_labels, word_start_tokens(tokenized)):\n",
    "        if is_word_start:\n",
    "            word_labels.append(label)\n",
    "    return word_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that out on a couple of example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    'Paris, the capital of France, is known for its iconic Eiffel Tower.',\n",
    "    'Elon Musk is the CEO of SpaceX and Tesla.',\n",
    "    'The Great Barrier Reef is the world\\'s largest coral reef system, located in Australia.',\n",
    "    'Mount Everest, the highest peak in the world, is part of the Himalayan mountain range.',\n",
    "    'The Mona Lisa, painted by Leonardo da Vinci, is displayed in the Louvre Museum in Paris.',\n",
    "    'Barack Obama served as the 44th President of the United States from 2009 to 2017.',\n",
    "    'The Amazon rainforest, often called the \"lungs of the Earth,\" is vital for global oxygen production.',\n",
    "    'Albert Einstein, a renowned physicist, developed the theory of relativity.',\n",
    "    'The Nile River is the longest river in Africa, flowing through multiple countries.',\n",
    "]\n",
    "\n",
    "\n",
    "for e in example_sentences:\n",
    "    words = e.split()    # Note: assumes white-space tokenization is OK\n",
    "    ner_tags = predict_ner(words)\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        print(f'{word:10s} âž” {tag}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "The IOB notation can be a bit tricky to interpret. To get a better intuitive understanding of tagging results, let's implement a visualization using the[`displacy`](https://explosion.ai/demos/displacy-ent) library.\n",
    "\n",
    "The code here mostly maps the IOB tags to character offets and formats the data for displacy. Unless you're interested in modifying this or otherwise working with this library, there's no need to go through this in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "\n",
    "# Mapping of MultiNERD types for displacy\n",
    "type_map = {\n",
    "    'PER': 'Person',\n",
    "    'LOC': 'Location',\n",
    "    'ORG': 'Organization',\n",
    "    'ANIM': 'Animal',\n",
    "    'DIS': 'Disease',\n",
    "}\n",
    "\n",
    "def render_with_displacy(words, tags):\n",
    "    tagged, offset, start, label = [], 0, None, None\n",
    "    for word, tag in zip(words, tags):\n",
    "        if tag[0] in 'OB' and start is not None:    # current ends\n",
    "            tagged.append({\n",
    "                'start': start,\n",
    "                'end': offset,\n",
    "                'label': type_map.get(label, label)\n",
    "            })\n",
    "            start, label = None, None\n",
    "        if tag[0] == 'B':\n",
    "            start, label = offset, tag[2:]\n",
    "        elif tag[0] == 'I':\n",
    "            if start is None:    # I without B, but nevermind\n",
    "                start, label = offset, tag[2:]\n",
    "        else:\n",
    "            assert tag == 'O', 'unexpected tag {}'.format(tag)\n",
    "        offset += len(word) + 1    # +1 for space\n",
    "    if start:    # span open at sentence end\n",
    "        tagged.append({\n",
    "                'start': start,\n",
    "                'end': offset,\n",
    "                'label': type_map.get(label, label)\n",
    "        })\n",
    "    doc = {\n",
    "        'text': ' '.join(words),\n",
    "        'ents': tagged\n",
    "    }\n",
    "    displacy.render(doc, style='ent', jupyter=True, manual=True)\n",
    "\n",
    "\n",
    "for e in example_sentences:\n",
    "    words = e.split()    # Note: assumes white-space tokenization is OK\n",
    "    ner_tags = predict_ner(words)\n",
    "    render_with_displacy(words, ner_tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Method to Apply to Validation Dataset (& Then Apply it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to('cpu')\n",
    "    attention_mask = batch[\"attention_mask\"].to('cpu')\n",
    "    labels = batch[\"labels\"].to('cpu')\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = trainer.model(input_ids, \n",
    "                               attention_mask\n",
    "                               )\n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        predicted_label = torch.argmax(output.logits, \n",
    "                                       axis=-1\n",
    "                                       ).cpu().numpy()\n",
    "        \n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, 11), \n",
    "                         labels.view(-1), \n",
    "                         reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Above Function to Entire Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forward_pass_with_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/srv/users/rudxia/Developer_NLP/POS Project with Wikineural Dataset - XLNet Transformer.ipynb Cell 43\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/POS%20Project%20with%20Wikineural%20Dataset%20-%20XLNet%20Transformer.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m eval_set \u001b[39m=\u001b[39m encoded_ds[\u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/POS%20Project%20with%20Wikineural%20Dataset%20-%20XLNet%20Transformer.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m eval_set \u001b[39m=\u001b[39m eval_set\u001b[39m.\u001b[39mmap(forward_pass_with_label,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/POS%20Project%20with%20Wikineural%20Dataset%20-%20XLNet%20Transformer.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                         batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/POS%20Project%20with%20Wikineural%20Dataset%20-%20XLNet%20Transformer.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                         batch_size\u001b[39m=\u001b[39m\u001b[39m24\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkistrand01.ki.se/srv/users/rudxia/Developer_NLP/POS%20Project%20with%20Wikineural%20Dataset%20-%20XLNet%20Transformer.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m eval_df \u001b[39m=\u001b[39m eval_set\u001b[39m.\u001b[39mto_pandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'forward_pass_with_label' is not defined"
     ]
    }
   ],
   "source": [
    "eval_set = encoded_ds['test']\n",
    "\n",
    "eval_set = eval_set.map(forward_pass_with_label,\n",
    "                        batched=True,\n",
    "                        batch_size=32)\n",
    "\n",
    "eval_df = eval_set.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Up Padding Tokens\n",
    "\n",
    "Defined a placeholder label ID for special tokens (e.g. `[IGN]`) and tokens that represent continuation wordpieces. For example, if the word `Partition` is tokenized into the parts `Part` and `##ition`, the subword token `##ition` would get this ID.\n",
    "\n",
    "(Here the \"magic\" value -100 is significant: this matches the default pythorch `ignore_index`, a value that is ignored in loss functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2014, 3942, 2000, 10722, 10175, 2226, 20...</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, O, O, B-LOC, IGN, IGN, O, O, O, O, O,...</td>\n",
       "      <td>[0.0, 5.5073175e-05, 5.4953973e-05, 0.00020072...</td>\n",
       "      <td>[O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, O, O, ...</td>\n",
       "      <td>[a, â–option, â–Governor, â–entered, â–Give, brook...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids   \n",
       "0  [101, 2014, 3942, 2000, 10722, 10175, 2226, 20...  \\\n",
       "\n",
       "                                      token_type_ids   \n",
       "0  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \\\n",
       "\n",
       "                                      attention_mask   \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \\\n",
       "\n",
       "                                              labels   \n",
       "0  [IGN, O, O, O, B-LOC, IGN, IGN, O, O, O, O, O,...  \\\n",
       "\n",
       "                                                loss   \n",
       "0  [0.0, 5.5073175e-05, 5.4953973e-05, 0.00020072...  \\\n",
       "\n",
       "                                     predicted_label   \n",
       "0  [O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, O, O, ...  \\\n",
       "\n",
       "                                        input_tokens  \n",
       "0  [a, â–option, â–Governor, â–entered, â–Give, brook...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[-100] = \"IGN\"\n",
    "eval_df[\"input_tokens\"] = eval_df[\"input_ids\"].apply(\n",
    "    lambda x: tokenizer.convert_ids_to_tokens(x))\n",
    "eval_df[\"predicted_label\"] = eval_df[\"predicted_label\"].apply(\n",
    "    lambda x: [id2label[i] for i in x])\n",
    "eval_df[\"labels\"] = eval_df[\"labels\"].apply(\n",
    "    lambda x: [id2label[i] for i in x])\n",
    "eval_df['loss'] = eval_df.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "eval_df['predicted_label'] = eval_df.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "eval_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unwrap Each Token Within Sample Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>â–option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3942</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>â–Governor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>â–entered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10722</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>â–Give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>â–boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>â–Program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2460</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>ab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids token_type_ids attention_mask labels  loss predicted_label   \n",
       "0      2014              0              1      O   0.0               O  \\\n",
       "0      3942              0              1      O   0.0               O   \n",
       "0      2000              0              1      O   0.0               O   \n",
       "0     10722              0              1  B-LOC   0.0           B-LOC   \n",
       "0      2001              0              1      O   0.0               O   \n",
       "0      3013              0              1      O   0.0               O   \n",
       "0      2460              0              1      O   0.0               O   \n",
       "\n",
       "  input_tokens  \n",
       "0      â–option  \n",
       "0    â–Governor  \n",
       "0     â–entered  \n",
       "0        â–Give  \n",
       "0         â–boy  \n",
       "0     â–Program  \n",
       "0           ab  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df_tokens = eval_df.apply(pd.Series.explode)\n",
    "eval_df_tokens = eval_df_tokens.query(\"labels != 'IGN'\")\n",
    "eval_df_tokens[\"loss\"] = eval_df_tokens[\"loss\"].astype(float).round(2)\n",
    "eval_df_tokens.head(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Which Tokens Have Accumulated Most Loss in Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_tokens</th>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–baby</td>\n",
       "      <td>â–distinction</td>\n",
       "      <td>â–survived</td>\n",
       "      <td>â–2004</td>\n",
       "      <td>â–enter</td>\n",
       "      <td>â–Clinton</td>\n",
       "      <td>â–county</td>\n",
       "      <td>â–removed</td>\n",
       "      <td>â–Vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17506</td>\n",
       "      <td>7366</td>\n",
       "      <td>104</td>\n",
       "      <td>48</td>\n",
       "      <td>486</td>\n",
       "      <td>114</td>\n",
       "      <td>1730</td>\n",
       "      <td>62</td>\n",
       "      <td>57</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.038</td>\n",
       "      <td>2.275</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.916</td>\n",
       "      <td>1.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>323.04</td>\n",
       "      <td>280.12</td>\n",
       "      <td>236.62</td>\n",
       "      <td>198.7</td>\n",
       "      <td>115.74</td>\n",
       "      <td>79.19</td>\n",
       "      <td>78.37</td>\n",
       "      <td>52.68</td>\n",
       "      <td>52.2</td>\n",
       "      <td>51.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0       1             2          3       4       5   \n",
       "input_tokens  â–ordered   â–baby  â–distinction  â–survived   â–2004  â–enter  \\\n",
       "count            17506    7366           104         48     486     114   \n",
       "mean             0.018   0.038         2.275       4.14   0.238   0.695   \n",
       "sum             323.04  280.12        236.62      198.7  115.74   79.19   \n",
       "\n",
       "                     6        7         8      9  \n",
       "input_tokens  â–Clinton  â–county  â–removed    â–Vi  \n",
       "count             1730       62        57     36  \n",
       "mean             0.045     0.85     0.916  1.432  \n",
       "sum              78.37    52.68      52.2  51.57  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    eval_df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Which Label IDs Have Most Loss in Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4173</td>\n",
       "      <td>4555</td>\n",
       "      <td>3512</td>\n",
       "      <td>2370</td>\n",
       "      <td>5593</td>\n",
       "      <td>2963</td>\n",
       "      <td>5904</td>\n",
       "      <td>234662</td>\n",
       "      <td>4107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.393</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>1639.1</td>\n",
       "      <td>1536.22</td>\n",
       "      <td>1089.68</td>\n",
       "      <td>552.99</td>\n",
       "      <td>1248.04</td>\n",
       "      <td>625.31</td>\n",
       "      <td>1138.46</td>\n",
       "      <td>6186.3</td>\n",
       "      <td>107.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2       3        4       5        6       7   \n",
       "labels  B-MISC   I-MISC    B-ORG   I-LOC    B-PER   I-ORG    B-LOC       O  \\\n",
       "count     4173     4555     3512    2370     5593    2963     5904  234662   \n",
       "mean     0.393    0.337     0.31   0.233    0.223   0.211    0.193   0.026   \n",
       "sum     1639.1  1536.22  1089.68  552.99  1248.04  625.31  1138.46  6186.3   \n",
       "\n",
       "             8  \n",
       "labels   I-PER  \n",
       "count     4107  \n",
       "mean     0.026  \n",
       "sum     107.08  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    eval_df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    "    .fillna(0)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Function to Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_token_list = list(set(eval_df_tokens['labels']))\n",
    "\n",
    "plot_confusion_matrix(eval_df_tokens[\"labels\"], eval_df_tokens[\"predicted_label\"],\n",
    "                      eval_token_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define & Call Function to Display Example Token Sequences Along With Labels & Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–strength</td>\n",
       "      <td>â–calling</td>\n",
       "      <td>â–periods</td>\n",
       "      <td>â–5,000</td>\n",
       "      <td>â–attend</td>\n",
       "      <td>threatening</td>\n",
       "      <td>â–quick</td>\n",
       "      <td>nan</td>\n",
       "      <td>â–Afghan</td>\n",
       "      <td>â–spirit</td>\n",
       "      <td>da</td>\n",
       "      <td>ther</td>\n",
       "      <td>â–Hampton</td>\n",
       "      <td>da</td>\n",
       "      <td>â–prospect</td>\n",
       "      <td>â–1999</td>\n",
       "      <td>â–determined</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>â–dynasty</td>\n",
       "      <td>â–prospect</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>â–words</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>nor</td>\n",
       "      <td>ition</td>\n",
       "      <td>â–composers</td>\n",
       "      <td>luck</td>\n",
       "      <td>â–Punjab</td>\n",
       "      <td>visible</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>18</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>â–soldier</td>\n",
       "      <td>â–Transportation</td>\n",
       "      <td>â–Give</td>\n",
       "      <td>â–teen</td>\n",
       "      <td>â–FM</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>18</td>\n",
       "      <td>â–mogul</td>\n",
       "      <td>â–baggage</td>\n",
       "      <td>fa</td>\n",
       "      <td>â–coroner</td>\n",
       "      <td>â–insurance</td>\n",
       "      <td>ji</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>â–napkin</td>\n",
       "      <td>fax</td>\n",
       "      <td>â–Rao</td>\n",
       "      <td>â–prospect</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>â–words</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>â–convinced</td>\n",
       "      <td>â–FBI</td>\n",
       "      <td>â–certify</td>\n",
       "      <td>ut</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>18</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>â–implications</td>\n",
       "      <td>â–aides</td>\n",
       "      <td>â–v</td>\n",
       "      <td>â–palace</td>\n",
       "      <td>cola</td>\n",
       "      <td>â–200</td>\n",
       "      <td>â–tried</td>\n",
       "      <td>18</td>\n",
       "      <td>ILL</td>\n",
       "      <td>ka</td>\n",
       "      <td>gia</td>\n",
       "      <td>â–seeking</td>\n",
       "      <td>â–Economic</td>\n",
       "      <td>â–forest</td>\n",
       "      <td>â–Electrical</td>\n",
       "      <td>â–Senegal</td>\n",
       "      <td>â–la</td>\n",
       "      <td>â–Township</td>\n",
       "      <td>18</td>\n",
       "      <td>â–agencies</td>\n",
       "      <td>â–crowded</td>\n",
       "      <td>â–Taste</td>\n",
       "      <td>â–outfielder</td>\n",
       "      <td>metric</td>\n",
       "      <td>â–magazine</td>\n",
       "      <td>â–cent</td>\n",
       "      <td>â–sculpture</td>\n",
       "      <td>Packard</td>\n",
       "      <td>â–themselves</td>\n",
       "      <td>â–insurance</td>\n",
       "      <td>â–north</td>\n",
       "      <td>â–so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.82</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.01</td>\n",
       "      <td>8.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1         2         3       4        5            6    \n",
       "tokens  â–ordered  â–strength  â–calling  â–periods  â–5,000  â–attend  threatening  \\\n",
       "labels         O          O     B-LOC         O       O        O            O   \n",
       "preds          O          O     B-ORG     I-ORG   I-ORG    I-ORG            O   \n",
       "losses      0.00       0.00      3.85      1.82    3.26     3.26         0.00   \n",
       "\n",
       "            7     8        9        10    11    12        13    14         15   \n",
       "tokens  â–quick   nan  â–Afghan  â–spirit    da  ther  â–Hampton    da  â–prospect  \\\n",
       "labels       O     O      IGN        O   IGN   IGN         O   IGN        IGN   \n",
       "preds        O     O        O        O     O     O         O     O          O   \n",
       "losses    0.00  0.00     0.00     0.00  0.00  0.00      0.00  0.00       0.00   \n",
       "\n",
       "           16           17      18        19         20      21      22   \n",
       "tokens  â–1999  â–determined  â–tried  â–dynasty  â–prospect  â–tried  â–words  \\\n",
       "labels      O            O       O    I-MISC     I-MISC       O       O   \n",
       "preds       O            O       O         O          O       O       O   \n",
       "losses   0.00         0.00    0.00      7.01       8.16    0.00    0.00   \n",
       "\n",
       "            23      24     25          26    27       28       29      30   \n",
       "tokens  â–tried     nor  ition  â–composers  luck  â–Punjab  visible  â–tried  \\\n",
       "labels       O       O    IGN         IGN     O      IGN      IGN       O   \n",
       "preds        O  I-MISC      O           O     O        O   I-MISC       O   \n",
       "losses    0.00    1.09   0.00        0.00  0.59     0.00     0.00    0.00   \n",
       "\n",
       "          31      32        33               34     35     36      37      38   \n",
       "tokens    18  â–tried  â–soldier  â–Transportation  â–Give  â–teen     â–FM  â–tried  \\\n",
       "labels     O       O    I-MISC              IGN      O    IGN     IGN       O   \n",
       "preds      O       O    I-MISC           I-MISC      O      O  I-MISC       O   \n",
       "losses  0.00    0.00      0.74             0.00   0.36   0.00    0.00    0.00   \n",
       "\n",
       "          39      40        41    42        43          44    45      46   \n",
       "tokens    18  â–mogul  â–baggage    fa  â–coroner  â–insurance    ji  â–tried  \\\n",
       "labels     O  I-MISC       IGN   IGN         O           O     O       O   \n",
       "preds      O       O         O     O         O           O     O       O   \n",
       "losses  0.00    1.55      0.00  0.00      0.17        0.00  0.00    0.00   \n",
       "\n",
       "             47    48    49         50      51      52      53          54   \n",
       "tokens  â–napkin   fax  â–Rao  â–prospect  â–tried  â–words  â–tried  â–convinced  \\\n",
       "labels   I-MISC   IGN   IGN     I-MISC       O       O       O           O   \n",
       "preds         O     O     O          O       O       O       O      I-MISC   \n",
       "losses     6.40  0.00  0.00       6.41    0.00    0.00    0.00        1.93   \n",
       "\n",
       "            55        56    57      58    59      60             61      62   \n",
       "tokens    â–FBI  â–certify    ut  â–tried    18  â–tried  â–implications  â–aides  \\\n",
       "labels     IGN         O   IGN       O     O       O              O     IGN   \n",
       "preds   I-MISC         O     O       O     O       O         I-MISC       O   \n",
       "losses    0.00      0.61  0.00    0.00  0.00    0.00           1.25    0.00   \n",
       "\n",
       "          63       64    65    66      67    68      69      70      71   \n",
       "tokens    â–v  â–palace  cola  â–200  â–tried    18     ILL      ka     gia  \\\n",
       "labels   IGN        O   IGN   IGN       O     O  I-MISC       O     IGN   \n",
       "preds      O        O     O     O       O     O  I-MISC  I-MISC  I-MISC   \n",
       "losses  0.00     0.23  0.00  0.00    0.00  0.00    0.18    0.90    0.00   \n",
       "\n",
       "              72         73       74           75        76    77         78   \n",
       "tokens  â–seeking  â–Economic  â–forest  â–Electrical  â–Senegal   â–la  â–Township  \\\n",
       "labels       IGN          O      IGN          IGN       IGN   IGN          O   \n",
       "preds          O          O        O            O         O     O     I-MISC   \n",
       "losses      0.00       0.65     0.00         0.00      0.00  0.00       1.44   \n",
       "\n",
       "          79         80        81      82           83      84         85   \n",
       "tokens    18  â–agencies  â–crowded  â–Taste  â–outfielder  metric  â–magazine  \\\n",
       "labels     O     I-MISC       IGN     IGN          IGN     IGN        IGN   \n",
       "preds      O          O         O       O            O       O          O   \n",
       "losses  0.00       1.77      0.00    0.00         0.00    0.00       0.00   \n",
       "\n",
       "            86          87       88           89          90      91    92  \n",
       "tokens   â–cent  â–sculpture  Packard  â–themselves  â–insurance  â–north   â–so  \n",
       "labels  I-MISC         IGN      IGN       I-MISC           O       O   IGN  \n",
       "preds        O           O        O       I-MISC           O       O     O  \n",
       "losses    1.99        0.00     0.00         0.14        0.00    0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–insertion</td>\n",
       "      <td>lit</td>\n",
       "      <td>eu</td>\n",
       "      <td>â–managed</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–retail</td>\n",
       "      <td>â–baby</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–feat</td>\n",
       "      <td>18</td>\n",
       "      <td>â–Jones</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–succeed</td>\n",
       "      <td>â–ammunition</td>\n",
       "      <td>â–critical</td>\n",
       "      <td>â–bid</td>\n",
       "      <td>â–evening</td>\n",
       "      <td>â–destruction</td>\n",
       "      <td>â–beer</td>\n",
       "      <td>â–V</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>mentary</td>\n",
       "      <td>18</td>\n",
       "      <td>â–grave</td>\n",
       "      <td>18</td>\n",
       "      <td>â–disappointing</td>\n",
       "      <td>18</td>\n",
       "      <td>â–proposals</td>\n",
       "      <td>18</td>\n",
       "      <td>rus</td>\n",
       "      <td>18</td>\n",
       "      <td>â–usual</td>\n",
       "      <td>18</td>\n",
       "      <td>pens</td>\n",
       "      <td>18</td>\n",
       "      <td>â–flames</td>\n",
       "      <td>18</td>\n",
       "      <td>â–offending</td>\n",
       "      <td>18</td>\n",
       "      <td>â–DVD</td>\n",
       "      <td>18</td>\n",
       "      <td>â–Jones</td>\n",
       "      <td>â–rushing</td>\n",
       "      <td>â–north</td>\n",
       "      <td>â–so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1     2     3         4         5        6      7    \n",
       "tokens  â–ordered  â–insertion   lit    eu  â–managed  â–ordered  â–retail  â–baby  \\\n",
       "labels         O           O     O     O         O         O        O      O   \n",
       "preds          O      B-MISC     O     O         O         O        O      O   \n",
       "losses      0.00        3.98  0.00  0.00      0.00      0.00     0.03   0.01   \n",
       "\n",
       "              8      9     10      11        12        13           14   \n",
       "tokens  â–ordered  â–feat    18  â–Jones  â–ordered  â–succeed  â–ammunition  \\\n",
       "labels         O  B-PER     O       O         O         O            O   \n",
       "preds          O      O     O       O         O         O            O   \n",
       "losses      0.00   4.26  0.00    0.00      0.00      0.00         0.00   \n",
       "\n",
       "               15    16        17            18     19    20        21   \n",
       "tokens  â–critical  â–bid  â–evening  â–destruction  â–beer    â–V  â–ordered  \\\n",
       "labels          O     O         O             O      O     O         O   \n",
       "preds           O     O         O             O      O     O         O   \n",
       "losses       0.00  0.00      0.00          0.00   0.00  0.00      0.00   \n",
       "\n",
       "             22    23      24    25              26    27          28    29   \n",
       "tokens  mentary    18  â–grave    18  â–disappointing    18  â–proposals    18  \\\n",
       "labels   B-MISC     O  B-MISC     O          B-MISC     O      B-MISC     O   \n",
       "preds         O     O       O     O               O     O           O     O   \n",
       "losses     1.69  0.00    2.29  0.00            2.86  0.00        3.74  0.00   \n",
       "\n",
       "            30    31      32    33      34    35       36    37          38   \n",
       "tokens     rus    18  â–usual    18    pens    18  â–flames    18  â–offending  \\\n",
       "labels  B-MISC     O  B-MISC     O  B-MISC     O   B-MISC     O      B-MISC   \n",
       "preds        O     O       O     O       O     O        O     O           O   \n",
       "losses    4.46  0.00    4.38  0.00    4.61  0.00     3.62  0.00        4.03   \n",
       "\n",
       "          39      40    41      42        43      44    45  \n",
       "tokens    18    â–DVD    18  â–Jones  â–rushing  â–north   â–so  \n",
       "labels     O  B-MISC     O       O     B-PER       O   IGN  \n",
       "preds      O       O     O       O         O       O     O  \n",
       "losses  0.00    5.16  0.00    0.00      6.21    0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>â–declined</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–defence</td>\n",
       "      <td>â–Lord</td>\n",
       "      <td>cy</td>\n",
       "      <td>â–multi</td>\n",
       "      <td>â–hundreds</td>\n",
       "      <td>â–Database</td>\n",
       "      <td>18</td>\n",
       "      <td>â–1999</td>\n",
       "      <td>â–Because</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–DVD</td>\n",
       "      <td>â–downward</td>\n",
       "      <td>â–influenza</td>\n",
       "      <td>gy</td>\n",
       "      <td>â–Electrical</td>\n",
       "      <td>â–Poly</td>\n",
       "      <td>ji</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–influenza</td>\n",
       "      <td>gy</td>\n",
       "      <td>â–Electrical</td>\n",
       "      <td>â–Poly</td>\n",
       "      <td>â–baby</td>\n",
       "      <td>â–freshman</td>\n",
       "      <td>â–Fire</td>\n",
       "      <td>â–sets</td>\n",
       "      <td>â–creating</td>\n",
       "      <td>â–words</td>\n",
       "      <td>â–ordered</td>\n",
       "      <td>â–contains</td>\n",
       "      <td>â–baby</td>\n",
       "      <td>â–difference</td>\n",
       "      <td>â–dozens</td>\n",
       "      <td>â–adds</td>\n",
       "      <td>â–mute</td>\n",
       "      <td>ang</td>\n",
       "      <td>â–type</td>\n",
       "      <td>â–Vienna</td>\n",
       "      <td>â–moon</td>\n",
       "      <td>â–baby</td>\n",
       "      <td>â–responses</td>\n",
       "      <td>â–Google</td>\n",
       "      <td>â–insurance</td>\n",
       "      <td>â–north</td>\n",
       "      <td>â–so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.36</td>\n",
       "      <td>9.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2      3     4       5          6    \n",
       "tokens  â–declined  â–ordered  â–defence  â–Lord    cy  â–multi  â–hundreds  \\\n",
       "labels          O         O         O      O     O       O          O   \n",
       "preds           O         O         O      O     O       O          O   \n",
       "losses       0.00      0.00      0.00   0.00  0.00    0.00       0.00   \n",
       "\n",
       "               7     8      9         10        11      12         13   \n",
       "tokens  â–Database    18  â–1999  â–Because  â–ordered    â–DVD  â–downward  \\\n",
       "labels          O     O      O         O         O  B-MISC        IGN   \n",
       "preds           O     O      O         O         O       O          O   \n",
       "losses       0.00  0.00   0.00      0.00      0.00    3.09       0.00   \n",
       "\n",
       "                14    15           16     17    18        19          20   \n",
       "tokens  â–influenza    gy  â–Electrical  â–Poly    ji  â–ordered  â–influenza  \\\n",
       "labels      I-MISC   IGN          IGN    IGN     O         O      B-MISC   \n",
       "preds            O     O            O      O     O         O           O   \n",
       "losses        6.24  0.00         0.00   0.00  0.00      0.00        6.50   \n",
       "\n",
       "          21           22     23      24         25     26      27         28   \n",
       "tokens    gy  â–Electrical  â–Poly   â–baby  â–freshman  â–Fire   â–sets  â–creating  \\\n",
       "labels   IGN          IGN    IGN  I-MISC     I-MISC    IGN  I-MISC        IGN   \n",
       "preds      O            O      O       O      B-PER  I-PER   I-PER      I-PER   \n",
       "losses  0.00         0.00   0.00    8.36       9.87   0.00   10.45       0.00   \n",
       "\n",
       "            29        30         31     32           33       34     35   \n",
       "tokens  â–words  â–ordered  â–contains  â–baby  â–difference  â–dozens  â–adds  \\\n",
       "labels       O         O          O      O            O        O    IGN   \n",
       "preds        O         O          O      O            O        O      O   \n",
       "losses    0.00      0.00       0.00   0.00         0.00     0.00   0.00   \n",
       "\n",
       "           36    37     38       39     40     41          42       43   \n",
       "tokens  â–mute   ang  â–type  â–Vienna  â–moon  â–baby  â–responses  â–Google  \\\n",
       "labels    IGN   IGN      O        O      O      O           O      IGN   \n",
       "preds       O     O      O        O      O      O      B-MISC        O   \n",
       "losses   0.00  0.00   0.00     0.00   0.00   0.00        1.29     0.00   \n",
       "\n",
       "                44      45    46  \n",
       "tokens  â–insurance  â–north   â–so  \n",
       "labels           O       O   IGN  \n",
       "preds            O       O     O  \n",
       "losses        0.00    0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        eval_df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n",
    "                               \"preds\": preds, \"losses\": losses}).T\n",
    "        yield eval_df_tmp\n",
    "\n",
    "eval_df[\"total_loss\"] = eval_df[\"loss\"].apply(sum)\n",
    "eval_df_tmp = eval_df.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "for sample in get_samples(eval_df_tmp):\n",
    "    display(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
